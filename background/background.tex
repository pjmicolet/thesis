\chapter{Background}~\label{chp:Background}

This chapter covers the different topics that are present in this thesis and is structured as followed:
\begin{itemize}
\item Sections~\ref{sec:cmp} and \ref{sec:hcmp} cover Chip Multicore Processors and Heterogeneous Chip Multicore Processors to motivate the use of Dynamic Multicore Processors (DMP).
\item Section~\ref{sec:dmp}  describes the three types of DMP that currently exists.
\item Section~\ref{chp:bg:sec:edge} describes the Explicit Data Graph Execution (EDGE) instruction set architecture (ISA) used by the DMP explored throughout this thesis.
Subsection~\ref{sec:edge_isa} explains how EDGE blocks are formed, which is useful for Chapter~\ref{chp:streamit} and Chapter~\ref{chp:cases}.
Subsection~\ref{sec:edge_arch} covers how blocks are executed on a single core, as well how they are executed when the processor is reconfigured, this is important knowledge for Chapters~\ref{chp:streamit}, \ref{chp:cases} and \ref{chp:hardchanges}.
\item Section~\ref{sec:valpred} explains how value predictors works, this is used in Chapter~\ref{chp:hardchanges}.
\item Section~\ref{sec:bg:stream} describes streaming programming languages, explored in Chapter~\ref{chp:streamit}.
\item Section~\ref{sec:ml} covers the different machine learning techniques and evaluation methods used.
Linear regression, described in subsection~\ref{sec:lr} is used in both Chapter~\ref{chp:streamit} and \ref{chp:cases}.
k Nearest Neighbors explained in subsection~\ref{sec:kNN} is used in Chapter~\ref{chp:streamit}, k-means clustering found at subsection~\ref{sec:kMeans} is used in Chapter~\ref{chp:cases}.
Leave-one cross validation described in subsection~\ref{sec:valid} is used in Chapter~\ref{chp:streamit} and \ref{chp:cases}.
\item Finally, Section~\ref{sec:esc} explains the Early Stopping Criterion used in Chapter~\ref{chp:streamit}.
\end{itemize}

\section{Chip Multicore Processors}~\label{sec:cmp}

\begin{figure}[t]
 \center
 \includegraphics[width=1\textwidth]{background/graphics/i7intel.pdf}
 \caption{Intel Core i7 processor internal die photograph taken from~\cite{turleywhite}}\label{fig:i7}
\end{figure}
 
Chip Multicore Processors (CMPs) are now ubiquitous due to the difficulty in scaling single core performance.
In a CMP, multiple processor cores are packaged on a single die as seen in Figure~\ref{fig:i7}.
The most commonly adopted CMP design features homogeneous cores as it reduces the design complexity from a hardware perspective~\cite{asanovic2006landscape}.
In a CMP the performance improvement come from running multiple tasks in parallel.
These tasks can either be different programs or multiple threads from a single program executing on multiple cores.

The performance benefits of executing a program on a CMP can be estimated using Amdahl's Law~\cite{amdahl1967validity}.
It states that the speedup \textit{S} obtained by executing a program on \textit{n} cores depends on the fraction of work which is parallelisable \textit{f}.

\begin{equation}
S = \frac{1}{(1-f) + \frac{f}{n}}
\end{equation}\label{amdlaw}

If a CMP features an infinite number of cores~\cite{eyerman2010amdahl}, then Amdahl's law can be rewritten as:

\begin{equation}
\lim_{n\to\infty} S = \frac{1}{(1-f)}
\end{equation}

Therefore, given any program, the speedup obtained through using a CMP is limited to the fraction \textit{f} of parallel work found in the program.
As all the cores are homogeneous, if the parallel fraction is low, then this causes serial bottlenecks to reduce the potential speedup, as no core will be specialised for single-threaded execution.
%This implication has pushed research into finding ways of parallelising code to its fullest~\cite{}, however this may not always be possible~\cite{}.
%Thus whilst CMPs have become a mainstain in processor design, the homogeneous model has its limits.
\section{Heterogeneous Chip Multicore Processors}~\label{sec:hcmp}

\begin{figure}[t]
 \center
 \includegraphics[width=1\textwidth]{background/graphics/biglittle.png}
 	\vspace{-3em}

 \caption{Example of a heterogeneous multicore processor~\cite{armbig}}\label{fig:blarm}
\end{figure}
\vspace{-1em}

Unlike CMPs, Heterogeneous Chip Multicore Processors (HCMPs) bring a variety of cores onto a single package.
This variety may come in different forms, such as having multiple instruction set architectures (ISA) on the same system on chip (SoCs)~\cite{venkat2014harnessingisa,venkatHipstr2016}, different size cores on an SoC~\cite{armbig,jeff2012big}.
Figure~\ref{fig:blarm} shows a schemata for ARM's big.LITTLE HCMP, where a high-performance Cortex-A15 is paired with a simpler, power efficient Cortex-A7.
The two cores are connected via a cache coherent interconnect which provides data coherence at the bus-level, allowing the cores to make reads to its neighbour~\cite{armbig}.
Software is then executed on one of the cores depending on the user's requirement.

Unlike CMPs, the variety of cores provide a flexibility to the hardware.
This can be used for different purposes, such as security~\cite{venkatHipstr2016}, energy/power savings and speeding up applications~\cite{venkat2014harnessingisa}.
Whilst the hardware diversity in HCMPs is an advantage compared to CMPs, it increases the complexity of the optimisation space.
For example, Gupta et al. in ~\cite{Gupta2017Dypo} show that a single-ISA octa-core big.LITTLE architecture can have 20 different CPU core configurations, combined with the ability to dynamically modify the voltage, this leads to 4000 unique possible hardware configurations to choose from at runtime.
This highly increases the complexity of obtaining the correct settings for different programs.
MPSoCs also face a similar issue as having more than a single ISA not only adds design challenges, but program migration between different cores may in fact deteriorate performance~\cite{DeVuystMigration2012}.
%In their 2014 paper, Venkat et al.~\cite{venkat2014harnessingisa} demonstrate that a multi-ISA HCMP can improve performance by up to 1.4x and achieve energy savings of up to 40\% compared to a CMP on a peak-power budget of 40W.
%They motivate the idea that HCMPs with heterogeneous ISAs even improve over the performance of single-ISA HCMPs with speedups around 15\% and energy savings of 21.5\%.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{background/graphics/cmp_new.pdf}
	\vspace{-3em}
    \caption{High-level view of a DMP that can modify its core count.}
    \label{fig:dynmulticore}
	\vspace{-0.5em}
\end{figure}

\vspace{-1em}
\section{Dynamic Multicore Processors}~\label{sec:dmp}
\vspace{-1em}

In both CMPs and HCMPs, the design of the processor is fixed, meaning that many of the trade-offs between power, performance and area cannot be changed after production.
Dynamic Multicore Processors (DMPs) attempt to bridge the gap between the two previous designs by allowing the execution substrate to adapt dynamically at runtime.
Mitall's survey ~\cite{MittalSurv2016} defines three types of modifiable resources: the core count~\cite{ipek2007CoreFusion,kim2007tflex,pricopi2012bahurupi}, number of resources that each core has~\cite{Homayoun3DPooling2012} and microarchitectural features~\cite{fallinhetblock2014,BauerRSE08,tavanaElastic}.

\paragraph*{Core Composition Dynamic Multicore Processors}


A DMP that can modify its core count is built out of homogeneous cores with a reconfigurable fabric on top.
Physical cores can function either on their own or as a group.
Figure~\ref{fig:dynmulticore} provides an high level view of a DMP: the physical cores \textit{P}, are composed into 3 separate compositions of sizes 2, 8 and 4.
Each of the compositions executes a single thread, leveraging the power of the cores that are grouped together.
A composition fetches instructions from a single source and executes them across all the physical cores that compose it.
Cores can fuse dynamically and create core compositions of any sizes.
The exact mechanism of core composition is described later on in Section~\ref{chp:bg:sec:edge}.

The advantage of a core composition DMP over the traditional CMP or HCMP is the ability to reconfigure the processor dynamically to better match the tasks at hand.
For example, large sequential sections of code with high Instruction Level Parallelism (ILP) can be accelerated on a core composition that mimics a wide superscalar processor.
On parallel workloads the DMP can be reconfigured by de-composing the composition as seen in Figure~\ref{fig:dynmulticore} to match the Thread Level Parallelism (TLP).

\paragraph*{Resource Sharing Dynamic Multicore Processors}
A more fine-grained DMP technique is resource-sharing.
For example in the WiDGET DMP by Watanabe et al. \cite{Watanabe2010Widget}, cores are built out of Instruction Engine front-ends which function similarly to Out of Order (OoO) cores' front and back end pipeline functions.
They then are connected to Execution Units which they can choose to use.
Each core in WiDGET also has access to their neighbours Execution Units, allowing for more variation.
Another example of resource sharing is Rodrigues et al.'s work~\cite{rodrigues2014perf} where a core can use resources such as Arithmetic Logic Units (ALUs) from other cores.

\paragraph*{Microarchitectural Reconfigurable Dynamic Multicore Processors}
A final example is a DMP which can reconfigure microarchitectural features to better fit the current application.
Fallin et al.~\cite{fallinhetblock2014} observe that serial code can exhibit phases that fit different microarchitectural features.
According to them, these phases may only been in the ten to hundred thousands instructions long.
These DMPs can therefore modify microarchitecural features, such as in-order or out-of-order execution, to best match the current phase of a program.

\section{EDGE}\label{chp:bg:sec:edge}

This section describes the Explicit Data Graph (EDGE) architecture which is the architecture used in this thesis.
First the ISA is covered, with an example of how source code is transformed into blocks of instructions.
Then the features of an EDGE processor are described, such as the ability of executing multiple blocks on a core and how core composition works with EDGE.

\subsection{EDGE Instruction Set Architecture}~\label{sec:edge_isa}

The Explicit Data Graph Execution (EDGE) architecture~\cite{burger04edge} is a dataflow architecture aimed at improving concurrency whilst being energy and power efficient \cite{smith2006edge,burger04edge}.
Similar to very long instruction word (VLIW) architectures that pack multiple sub-instructions into a single instruction, EDGE requires that the compiler pack instructions into blocks.
Unlike VLIW that uses static placement and static issue -- which high pressure on the compiler -- EDGE allows for dynamic issue which distributes responsibility between the hardware and the compiler more evenly.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{background/graphics/EDGE_4.pdf}
    \caption{High-level view of the two main EDGE compiler passes. First pass involves standard optimisations, and code generation. Second pass transforms the code into atomic blocks of instructions.}
    \label{fig:EdgeHigh}
\end{figure}
\paragraph*{Block Formation}
In EDGE, instructions are organised into blocks, which are fetched as single units by the processor.
Figure~\ref{fig:EdgeHigh} shows a high-level overview of how EDGE creates the blocks of instructions; this is a summary of the work by Smith et al.~\cite{smith2006edge}. 
The first pass (Code Generation) transforms source code into a control flow graph (CFG), performing optimisations such as loop unrolling and inlining.
Then, each node of the CFG is turned into an EDGE block given a set of restrictions.
These restrictions are:
\begin{itemize}
\item Block Size: an EDGE block may be between 4 to 128 instructions.
\vspace{-1em}
\item Load/Store: an EDGE block may have at most 32 load/store instructions.
\vspace{-1em}
\item Entry/Exit: an EDGE block may have a single exit but may have multiple exits.
\end{itemize}
If a block does not meet these requirements, it is broken down into smaller blocks.

Unlike traditional ISAs such as x86 or ARM, instructions in a block do not communicate via registers, but rather the output targets of instructions are encoded to instruction inputs~\cite{smith2006edge}.
Loads and stores in each EDGE block are assigned unique identifiers which are used resolve load-store dependencies.
Thus, the EDGE architecture encode dependencies between instructions at the ISA level.
Registers are only used for inter-communication between blocks.

An EDGE block also contains a header that will inform the hardware about the number of stores and register writes contained in the block.
This information is used to detect data-dependencies between blocks if multiple blocks are executed in parallel.
For example, if two blocks write to the same register, then the register-file can detect which one must fire the write first based on the header information. 

\paragraph*{Hyperblock formation}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{background/graphics/hyperblock.pdf}
    \caption{High-level view of hyperblock formation. The three top blocks can be fused into a single block using instruction predication.}
    \label{fig:EdgeHB}
	\vspace{-1em}
\end{figure}
To increase the size of EDGE blocks, multiple blocks can be combined together to form one large block called a hyperblock.
This is achieved through the use of instruction predication~\cite{smith2006edge}.
For example, the \textit{if/else} statement in the C code found in Figure~\ref{fig:EdgeHigh} normally generates two blocks, one for each statement.
However, using predication, the two blocks can be fused together and appended to the block that precedes the \textit{if/else}, reducing the number of blocks from 3 to 1.
Figure~\ref{fig:EdgeHB} shows the organisation of the new block, where the colour represent where the instructions originated.

As the compiler needs to declare stores and which registers are written to in the block header, extra instructions may need to be generated to ensure the block always executes the same amount of stores.
In the case of Figure~\ref{fig:EdgeHB}, the last instruction is generated to ensure that a write to the register \textit{g71} always happens regardless of the predicate.
This adds some complexity to hyperblock formation, as the size of the block may be artificially inflated if the merging blocks write to different registers or have a different number of stores.
Even so hyperblocks allow for increased instruction level parallelism (ILP) as  which in turn improves the performance of the application being executed~\cite{smith2006edge}.
Overall, the EDGE ISA enables the architecture to dispatch blocks speculatively, with low overhead~\cite{putnam2010e2,kim2007tflex}, therefore, increasing exploitation of ILP.

 \begin{figure}[t]
 \center
 \includegraphics[width=1\textwidth]{background/graphics/edge_lanes.pdf}
 \vspace{-4em}
 \caption{Example of a 4 lane core on an EDGE processor taken from~\cite{putnam2010e2}. The instruction window is split into 4 lanes, each can fetch a block of up to 32 instructions.}\label{fig:e2segment}
\vspace{-1em}
 \end{figure}

\subsection{EDGE Processor features}~\label{sec:edge_arch}

This section covers two important features of an implementation of EDGE processor called E2~\cite{putnam2010e2} which is used throughout the thesis.
These are the segmentation of the instruction window, and the core-composition mechanism used in later chapters.

\subsubsection{Core Lanes} 

According to Smith et al. in ~\cite{smith2006edge}, the sizes of blocks generated by the compiler for a set of applications from SPEC2000~\cite{spec2000} and EEMBC~\cite{eembc} range from 5 instructions to 50 instructions and on average are 20.6 instructions long.
If an EDGE processor is designed to fetch large blocks, which can be up to 128 instructions long, then the compiler is currently not able to generate large enough blocks.
To maximise core-utilisation, an EDGE processor instruction window can be segmented into lanes.
This allows the core to fetch more than a single block at a time.

Figure~\ref{fig:e2segment} shows a schemata for a four-lane EDGE core.
Given that a block in EDGE can be up to 128 instructions long, each lane is allowed to fetch a block of up to 32 instructions.
Fetching blocks larger than 32 instructions results in more than one lane being occupied by a single block.
Depending on the architectural setup, lanes may either share ALUs or they may have private ALUs.
As the instruction window is segmented, a core can continue fetching blocks until all its lanes are filled, allowing the processor to have multiple blocks in flight per core.
A segmented instruction window therefore allows EDGE cores to be more flexible to block size variability.

\subsubsection{Core Composition for EDGE}\label{chp:Background:sec:EDGE}

\paragraph*{Hardware implementation}
Core Composition is achieved by fusing a set of \textit{physical} cores to create a larger core composition.
In the processor used throughout the thesis this does not modify the physical structure of the processor, instead it provides a unified view of a group of physical cores to the software.
The micro-architecture is distributed: register files, Load Store Queues (LSQs), L1 caches and ALUs all look like nodes on a network.
This means that when cores are composed together, this is similar to adding an extra node to the network.
Core composition is a dynamic modification and may occur during the execution of a program to better fit the workload.
Unlike traditional CMPs, composed cores operate on the same thread and extract Instruction Level Parallelism (ILP) rather than Thread Level Parallelism (TLP)~\cite{micolet2016dmpstream,pricopi2012bahurupi}.

\begin{figure}
 \center
 \includegraphics[width=1\textwidth]{cases-paper/graphics/background/proc_test.pdf}
 \caption{Core Composition Mechanisms for the EDGE-based architecture.}\label{fig:dmp}
 \end{figure}
 
\paragraph*{Core composition mechanism}
Figure~\ref{fig:dmp} shows the different stages and mechanisms of core composition for a four core EDGE system.
When creating a new composition a master core informs all other cores about the fusion and sends the predicted next block address to an available fused core.
When a new thread is started on a fused core the OS and runtime write the new core mapping to a system register.
The hardware then flushes these cores if they are not idle and sets the PC of the first block of that thread on one core in the composition and starts executing.
When a core mispredicts a branch in a composition, it informs the other cores which flush any younger blocks.
When un-fusing, the master core informs the other cores, which then commit or flush their blocks and power down while the master core continues to fetch and execute blocks from the thread.
The extra hardware required to support dynamic reconfiguration is very minimal~\cite{kim2007tflex} since most of the machinery already in place can be reused such as the cache coherence protocol when fusing and un-fusing the cores.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{chapter3/graphics/normfetch.pdf}
    \caption{Example of the fetching model on a 2 core composition. Each core has 4 segments, the arrows represent the block generating the predictions.}
   \label{fig:old_fetch}
%\vspace{1em}
\end{figure}

\paragraph*{Fetching on a composition}
Cores in a composition do not fetch blocks independently; instead a core will only start fetching if it is told to do so by another core in a composition.
Figure~\ref{fig:old_fetch} illustrates the fetching mechanism on a 2 core composition where each core has its instruction window segmented into four lanes.
The first core, \textit{C0} initiates the composition and starts fetching blocks until it is full.
Once this happens, it will send a fetch request to \textit{C1}, which then, and only then, will start fetching blocks.
Core \textit{C0} stops fetching blocks after this request, and will have to wait on \textit{C1} to send it a request before it can restart fetching.

\paragraph*{Memory and register consistency} 
When a core composition has multiple blocks on different cores, it can execute the instructions out of order.
However instructions that modify memory or registers pass through the load store queue (LSQ) and register-file respectively and are executed in order to ensure memory and register value consistency.
Register reads from younger blocks that depend on a register write from an older block will be stalled until the write is executed.
In case of a memory violation caused by undetected dependencies a flush of all blocks younger than the violator, including the violating block, is performed.

\lstset{
	backgroundcolor=\color{lbcolor},
	tabsize=2,
	rulecolor=,
	language=matlab,
        basicstyle=\tiny,
        upquote=true,
        aboveskip={1\baselineskip},
        columns=fixed,
        showstringspaces=false,
        extendedchars=true,
        breaklines=true,
        prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
        frame=single,
        showtabs=false,
        showspaces=false,
        showstringspaces=false,
        identifierstyle=\ttfamily,
        keywordstyle=\color[rgb]{0,0,1},
        commentstyle=\color[rgb]{0.133,0.545,0.133},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
		numbers=left,
}


\setbox0=\hbox{%
\begin{minipage}{2.5in}
\begin{lstlisting}[
basicstyle={\tiny\ttfamily},
identifierstyle={\color{black}},
tabsize=2,
language={[AspectJ]Java},
numbersep=8pt,
numbers=left,
xleftmargin=0.5cm,frame=tlbr,framesep=2pt,framerule=0pt]
for(int i =0; i < max_num;i++)
	b[i] = i;
for(int i =0 ; i < max_num ; i++)
	a[i] = b[i];
\end{lstlisting}
\end{minipage}
}
\savestack{\listingA}{\box0}

\setbox0=\hbox{%
\begin{minipage}{2.5in}
\begin{lstlisting}[
basicstyle={\tiny\ttfamily},
identifierstyle={\color{black}},
tabsize=2,
language={[AspectJ]Java},
numbersep=8pt,
numbers=left,
xleftmargin=0.5cm,frame=tlbr,framesep=2pt,framerule=0pt,
morekeywords ={class,run}
]
loop2:
cmp r1, max_num
beq exit
lsli r7, r1, 2
add r3, r2, r7
add r5, r4, r7 
load r6, r3
store r6, r5
addi r1, r1, 1
br loop2
\end{lstlisting}
\end{minipage}
}
\savestack{\listingB}{\box0}

\section{Value Prediction}~\label{sec:valpred}
%This will go in the background most likely
In superscalar processors, instructions can be executed out of order (OoO) to improve ILP.
However, some instructions may depend on results from previous instructions, or the memory system, such as an L1 cache response.
In such a case, the instruction will be stalled until the data is available.
These data dependencies reduce the potential ILP, which in turn limits performance.

Value Prediction aims to provide a hardware solution to reducing latencies caused by memory dependencies in OoO superscalar processors~\cite{gabbayVPOrig} by predicting the values produced by instructions.
Instructions can be speculatively executed with predicted values as they wait for the real values to become available.
This is similar to branch prediction, where instructions from a predicted branch path are executed before the branch is resolved.
As long as the accuracy of the prediction is high, then executing the data-dependent instructions with speculative data increases ILP.

%Until recently the benefits of using value prediction has been overshadowed by the potential power consumption and design complexity required to implement them \cite{peraisVTAGE2014}.
%However, recent work by Perais et al.~\cite{peraisVTAGE2014,peraisBeBop2015} shows that by adopting the prediction and update scheme of the ITTAGE branch predictor~\cite{SeznecITTAGE}, and grouping predictions into blocks, value predictors be efficiently implemented.

\begin{figure}[t]
\begin{tabular}{|c|c|}
\hline
\stackinset{l}{-5pt}{t}{13\llength}{}{\listingA} &
\stackinset{l}{-5pt}{t}{ 7\llength}{}{\listingB} \\
\hline
\end{tabular}
\captionof{lstlisting}{Small C example, with the second loop's assembly in the right pane}~\label{lst:bg:lst}
\vspace{-2em}
\end{figure}

\paragraph*{Type of predictors}
There exist two categories of value predictors: \textit{Context} predictors and \textit{Computational} predictors.
To illustrate the difference between the two, Figure~\ref{fig:pred_examples} depicts how both generate a prediction for the load instruction at line 17 in Listing~\ref{lst:bg:lst}.
In Figure~\ref{fig:pred_examples}, the colour of an entry in the tables represents a value for a specific instruction based on its program counter (PC).
The context box represents some information that is used by the predictor, such as global history, or dataflow history.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{background/graphics/predic.pdf}
    \caption{Depiction of how predictions are made in a context and computational value predictor. The colours of the cells represent values for different instructions.}
    \label{fig:pred_examples}
	\vspace{-1em}
\end{figure}
A \textit{Context} predictor relies on the history of values an instruction outputs to generate a prediction.
This means that a single instruction can occupy multiple entries of the predictor as each new value must be recorded.
This is why, in Figure~\ref{fig:pred_examples}, the specific load instruction has multiple entries in the predictor, as the value generated by the load is different for each iteration of the loop.
The \textit{Context} predictor can only predict the value the instruction if the specific value is already stored in the predictor as it does not have any ability to \textit{infer} the value.
The predicted value depens on the context, which can either be captured by dataflow information or branch history~\cite{peraisVTAGE2014}.
Overall, whilst predictions may be simpler to generate they are considered space-inefficient~\cite{peraisBeBop2015}.

%This makes \textit{Context} predictors less efficient for predicting values that are incremented at a constant rate, as each increment occupies a new entry~\cite{peraisBeBop2015}.

On the other hand \textit{Computational} predictors make a prediction by computing the value.
This can be achieved by determining the delta between two successive values for an instruction, also known as the \textit{stride}.
Instead of storing each observed value for an instruction, a \textit{Computational} predictor stores the \textit{stride}, and the last committed value of an instruction.
When a prediction is requested, the \textit{Computational} predictor fetches the last committed value and the stride and adds them together to produce the prediction.
For example, the value of the array in listing~\ref{lst:bg:lst} is always equal to the previous load value $+$ 1.
This means that the \textit{stride} for that instruction is 1 as seen in Figure~\ref{fig:pred_examples}.
If the last seen value for that instruction was 5, then the next prediction will fetch the value 5 from the last value table, and then add the stride (1), to generate the prediction.
This allows \textit{Computational} to potentially reduce the size of the predictor as it only requires a single copy of the value to make a prediction. 


\subsection{The Differential Value Tagged Geometric length Predictor}~\label{chp:bck:vtage}

This section covers a specific implementation of a value predictor that is used in Chapter~\ref{chp:hardchanges}.
The Differential Value Tagged Geometric length Predictor (D-VTAGE)~\cite{peraisBeBop2015} is a Computational value predictor that operates on blocks of instructions to minimize network pressure and increase the prediction output per cycle.
To simplify the explanation of the predictor, the size of the block is set to 1 instruction.
D-VTAGE is organised in two parts, a Last Value Table (LVT) that contains the last committed values for a given set of instructions, and a set of value tables (VT) which contain the strides.
The left hand side of Figure~\ref{fig:stride_over} shows an overview of the predictor with N stride tables.
The largest stride table is a tagless base predictor that is directly indexed, whereas VT1 to VTN are tagged by hashing the Program Counter of the instruction with varying amounts of the branch history.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{background/graphics/valuepred.pdf}
	\vspace{-3em}
    \caption{VTAGE computational value predictor and Speculative Window.}
    \label{fig:stride_over}
	\vspace{-1em}
\end{figure}

\paragraph*{Confidence Counter}
A predicted value does not have to be used immediately, as the instruction can still be dispatched normally.
This means that the value predictor can be trained before a prediction is used.
In D-VTAGE, each entry is paired with a 3-bit saturating counter that is either incremented on a correct prediction, or set to 0 on a misprediction.
When a prediction is made, it is only used when the confidence counter is saturated to ensure high accuracy.
However, a 3-bit satured counter may not be enough to ensure that all used predictions are correct.

A method to increase accuracy without increasing the physical size of the counter is to use Forward Probabilistic Counters (FPC)~\cite{riley2006fpc}.
FPCs are essentially a compression technique for counters; instead of incrementing the counter by 1 each time a prediction is correct, it is incremented based on a probability.
For example, given a 3-bit counter and a probability vector  $\{1,\frac{1}{16},\frac{1}{16},\frac{1}{16},\frac{1}{16},\frac{1}{16},\frac{1}{32},\frac{1}{32}\}$, then the counter has a 100\% chance of being incremented from 0 to 1, whilst it only has a $\frac{1}{16}$ chance of incrementing from 1 to 2.
This effectively increases the value at which predictions will be used without physically increasing the size of the counter. 

\paragraph*{Making a prediction}
All the tables of the value predictor are accessed in parallel using the Program Counter hashed with different amounts of the branch history.
The prediction will fetch values from the LVT and the stride tables.
The base predictor and LVT are directly indexed, whereas the tables VT1 to VTN are tagged.
The stride information will come from the table that has the longest branch history.
The prediction is then generated by adding the stride to the value in the LVT.
If the confidence counter of the entry is equal to the firing value then the prediction is used, it not then the prediction is kept alive until the instruction is committed to train the predictor.

\paragraph*{Updating the predictor}
Predictions are validated at the commit phase of the instruction to simplify the hardware~\cite{peraisVTAGE2014}.
On a correct prediction, the entry that generated the prediction has its confidence counter incremented by one.
If the predictor mispredicts, the entry's confidence counter is reset to 0 and the entry is propagated to tables that use longer branch histories.
If the entries are propagated to tables with tags, then the PC and branch prediction history are hashed together to form the tag.
The prediction and update schemes are directly inherited from the Tagged Geometric (TAGE) branch predictor~\cite{SeznecITTAGE}.

\subsubsection{Speculative Window}
As predictions are updated at commit time, if multiple blocks with the same PC are live, this can potentially cause multiple value mis-predictions as the information in the value predictor may be stale.
To ensure this is not the case, current live predictions must be held in a speculative window so that new predictions may use the last predicted values.
The right hand side of figure~\ref{fig:stride_over} demonstrates how the speculative window works. 
Instead of directly querying the value predictor, the last value table and speculative window are searched in parallel for a matching tag which in this case is the Program Counter (PC) of the block.
If there is a live prediction, then the value from the speculative window is used instead of the predictor.

\section{Streaming Programming Languages}~\label{sec:bg:stream}

% % This section should explain what steaming programming is (remove all the details about each language)
% General purpose programming languages often propose very little support for programs that handle with a continuous flow of data.
% This results in having to design a set of complicated for loops to manage the streams of data.
% Having to deal with different rates of incoming and outcoming data also increases the complexity of writing these applications using a standard language.

Streaming programming languages are a branch of dataflow programming that focus on applications that deal with a constant stream of data.
These applications, such as audio or video decoding can be commonly found in mobile devices.
Unlike conventional programming languages such as C++, these languages abstract the concept of incoming and outgoing data to permit the programmer to focus on how the data should be treated.
Programs are described as directed graphs where nodes are functions and their edges represent their input and output streams. 
These languages offer primitives to describe such a graph~\cite{theis2002streamit} which expose parallelisable and serial sections of the application directly to the compiler. 
Rates of incoming and out-coming data can also be defined to facilitate load balancing optimisations~\cite{chen2005rawstream}.

Features of streaming programming languages make them an ideal language for targeting multicore processors.
The explicit data communication between the different tasks in the program, the ability to estimate the amount of work performed in each task and information about data rates between tasks allows the compiler to easily generate a multi-threaded application that can run on a dynamic multicore processor.
However, the main challenge consists of deciding how to map the different tasks onto threads and how to allocate the right amount of resources to maximize performance.


\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{streamit-paper/graphics/streamit_types.pdf}
    \caption{Visual representation of the three different StreamIt structures.}
    \label{fig:streamittypes}
		\vspace{-1em}

\end{figure}

\subsection{StreamIt Programming Language}\label{chp:bckg:streamit}
StreamIt~\cite{theis2002streamit} is a high-level synchronous dataflow streaming programming language that defines programs as directed graphs.
StreamIt offers an elegant way of describing streaming applications, abstracting away how infinite data streams are managed to allow the programmer to solely focus on how the data must be treated.
A StreamIt program is composed of functions - called \textit{Filters} - which operate on streams of data.
Filters declare a certain amount of data which is to be consumed and produced per schedule.
Filters can be connected via \textit{Pipelines}, \textit{SplitJoins} or \textit{Feedback Loops} to create the streaming application.

Figure~\ref{fig:streamittypes} displays the different methods of connections in detail.
Pipelines (Figure~\ref{fig:streamittypes}(a)) represent a sequence of connecting filters operating on the same stream, each filter in the stream will operate on the output of the previous filter.
In a SplitJoin (Figure~\ref{fig:streamittypes}(c)), data from the stream is passed through a split filter and is either duplicated and passed on in parallel to the filters or distributed amongst the filters in a round-robin fashion.
The output of all the filters in a SplitJoin are then concatenated in a round-robin fashion through a joiner filter.
Finally a Feedback Loop (Figure~\ref{fig:streamittypes}(b)) provides a way for filters to operate on their outputs.
The resulting program written in StreamIt represents a graph where the nodes are filters and their edges represent the incoming and outgoing data streams.

\section{Machine-learning techniques and evaluation}~\label{sec:ml}
This section covers the different machine learning techniques and evaluation model used throughout the thesis.
The three techniques discussed are: linear regression, k-Means Clustering and k-Nearest Neighbours, whilst the evaluation technique is "leave-one out" cross validation.

\subsection{Linear Regression}~\label{sec:lr}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{background/graphics/lin_reg_ex.pdf}
	\vspace{-1em}
    \caption{Example of linear regression. The inputs are represented by the circles, and the generated linear regression is represented by the red line.}
    \label{fig:linregex}
\end{figure}
Linear regression has previously been used to detect when to reconfigure hardware~\cite{tavanaElastic}.
Knowing when to reconfigure the processor necessitates fast decision making to minimize the overall cost of reconfiguration.
Machine learning models therefore need to be lightweight to ensure that the computations required to operate the model are as low as possible.
In these situations, a popular model is Linear Regression as it has a fairly low computational footprint~\cite{tavanaElastic}.

Linear Regression assumes a linear relationship between a set of inputs and the predicted output and generates a model in the form of:

\begin{equation}
y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n
\end{equation}

where y is the predicted output, $X_{1..n}$ are the inputs and $\beta_{0.n}$ are weighted regression coefficients.
When training a linear regression, it generates the $\beta$ weights as to minimize the square-error between the training inputs and predicted outputs.
Once a model has been generated from input data, it can be implemented as a set of sums in hardware, making it an efficient way of making predictions.
Figure~\ref{fig:linregex} shows an example of a linear regression on a small dataset.
The linear regression takes the form of $y=4.77 + 0.88 \times x$ and is represented by the red line.

\subsection{k-Means Clustering}~\label{sec:kMeans}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{background/graphics/smallest_clust_example.pdf}
    \caption{Small example of k-Means clustering with two clusters. The squares represent datapoints whilst the circles represent the centroids of the clusters.}
    \label{fig:kmeanex}
\end{figure}

k-Means is a clustering technique that groups \textit{n} observations into \textit{k} clusters such that $k < n$.
The original algorithm presented by Lloyd in ~\cite{kMeans} first randomly places \textit{k} centroids \textit{c} at random locations in the space.
Then, each point \textit{$x_{i}$} in the space of \textit{n} observations is assigned to a cluster who's centroid \textit{$c_{j}$} is closest to it.
Once each point has been assigned to a cluster, each centroid is recomputed by re-centering itself around the datapoints in the cluster.
This is repeated until the centroids no longer move.
 
Figure~\ref{fig:kmeanex} provides a visual example of a running k-Means on a toy dataset, with k set to two.
Each colour in the graph represents a cluster, the squares represent the n observations, whilst the circles represent the centroids.

\subsection{k Nearest Neighbours}~\label{sec:kNN}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{background/graphics/knn1.pdf}
    \caption{Small example of k Nearest Neighbours classification on a toy dataset with two clusters and k = 4. The squares represent datapoints of the trained kNN dataset. The figure shows a new point (circle) being classified.}
    \label{fig:knn}
\end{figure}
k Nearest Neighbours (kNN) can be used for both regression and classification.
In both situations an output is generated by averaging the \textit{k} nearest neighbours to the input from the training data.
The average is often obtained by a weighted sum of the features of the k neighbours.
Figure~\ref{fig:knn} shows an example of a kNN classification on a new point.
The squares represent the trained kNN dataset, the colours represent the different classifications.
When a new point is added (as seen on the right hand graph), the four closest neighbours are found, and it is classified under the same category.

\subsection{Leave One Out Cross Validation}~\label{sec:valid}
The process of leave one out cross validation involves using the training dataset as the evaluation dataset.
Given a set of $n$ points in the training set, a new model is trained using $n-1$ of the points available.
The last unused datapoint is evaluated against this new model.
This procedure is repeated for every point in the dataset, and the results of the evaluation are averaged out to produce a final result.
This validation model works well when the number of testing and training data is small.

\section{Early Stopping Criterion}~\label{sec:esc}

When exhaustive search of an optimisation space is not feasible, a subset of the space is explored.
The early stopping criterion (ESC)~\cite{vuduc2003AutomaticPerf} provides a method of determining when new points added in the subspace no longer provide new information, and thus the currently evaluated subspace is representative of the total space.
Given $N$ possible number of implementations, if $i$ is a single implementation and $x_i$ is its performance normalised between 0 and 1, then $S=\{x_1,...,x_N\}$ is the set of all performances for all possible implementations.
If $X$ is a random variable corresponding to a randomly selected performance from the set S then there is a cummulative distribution function (CDF) $F(x) = Pr[X \le x]$ which is equal to the number of items in S that are less than or equal to $x$ (the function $n(x)$) over all possible items $N$.
If, for a number of time steps, a performance from S is randomly chosen and not replaced, then at time step $t$ $M_t$ is maximum performance observed $max_{1\le i \le t} Xi$ up to time t.
ESC posits that, that if the probability that $M_t$ is within a distance of the maximum performance found in S then the space exploration can stop at time t.
Formally, ESC is:
\begin{equation}
Pr[M_t \le 1 - \epsilon ] < \alpha
\end{equation}

Where $\epsilon$ is the proximity to the best performance in $S$ and $\alpha$ is the uncertainty factor.
For example, if the user wants to stop the search when $M_t$ is within 5\% of the best, with an uncertainty factor of 10\% they would set $\epsilon$ to 0,05 and $\alpha$ to 0,1.

As ESC is used when the exhaustive space has not been searched, the set of all performances $S$ is unknown.
Instead, ESC uses an approximation of the space, using the set of already observed performances rather than $S$.
The approximation is determined via this formula:
\begin{equation}
\hat{G}_t(x) = \frac{ \binom{\ceil*{N \times \hat{F}_t(x)}}{t}}{\binom{N}{t}}
\end{equation}

Where $\hat{F}_t(x)$ is the CDF for the random variable X given the set of observed performances at time $t$.
Finally, using the approximation, ESC can be defined as:

\begin{equation}
\hat{G}_t(1-\epsilon) < \alpha
\end{equation}
