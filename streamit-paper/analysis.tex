This section describes the exploration of the software/hardware co-design space.
The software side includes partitioning the program, determining the number of threads and the loop unrolling compiler optimization.
The hardware side is about finding out the best core composition that maximizes performance for a given partitioning.

\subsection{Thread Partitioning}

This section first starts with analyzing the impact of thread partitioning on performance.
Thread partitioning is about deciding how many threads to create and how to partition StreamIt filters into these threads.
To simplify this study, the default streaming partitioner is used to decide on how to allocate filters to threads which is based on simulated annealing~\cite{simulatedAnnealing1983}.
On the hardware side, two scenarios are considered: the ``without composition scenario'' where there is exactly one core per thread and the ``with composition scenario'' where each thread receives between 1 and 15 cores.

Figure~\ref{fig:threadtrend} shows how performance varies under both scenarios as a function of the number of threads.
Regardless of how cores are composed it can be observed that curves for a benchmark follow the same trend.
The optimal number of threads using core composition is very similar to the scenario without composition.
This important observation means that the optimal number of threads for a benchmark can be estimated independently from the hardware composition.
The system can therefore proceed in two stages: first determine the optimal number of threads and then decide on a core composition.

Figure~\ref{fig:threadtrend} also shows that the performance of most benchmarks starts to deteriorate passed a certain number of threads making it critical to not over-allocate threads.
This number of threads varies between benchmarks, thus it motivates the use of machine learning to decide the optimal number of threads to use.
Finally it is important to observe that executions without compositions always perform worse.
This demonstrates that composing cores is essential to obtain the best performance from a workload.


\subsection{Core Composition}

\begin{figure}[h]
  \includegraphics[width=1\textwidth]{streamit-paper/graphics/audiobeam_tots.pdf}
  \caption{Distribution of Audiobeam performance when modifying the amount of threads and compositions.}\label{fig:audiototal}
\end{figure}

Using core composition, the processor fuses a number of cores and associates them to a thread to increase single threaded performance.
Whilst this flexibility is advantageous, choosing the right amount of cores for a given thread is difficult due to the large number of possible configurations~\cite{gulati2008multitaskingdmc}.

Figure~\ref{fig:audiototal} shows how threading and core-compositions affect performance for the \bench{Audiobeam} benchmark.
The curves represent the density distribution for different core compositions as a function of the number of threads.
The right hand side Y-axis represents the number of threads present in the current version of the benchmark whilst the left Y-Axis represents the density normalized by the total number of points in the design space.
For each of the threaded versions the benchmark runs using 100 different core-compositions.
The density curve for thread 15 is a single point as there exists only a single composition, so a line is drawn to represent where that point lies.

The width of each of the curves represents the influence of composition on the \bench{Audiobeam}'s performance for a given number of threads.
For this benchmark the impact of core composition is actually very large for the best performing number of threads (1--5).
For example, for two threads, the best configuration can lead to a 1.5x speedup compared to the worst point.
Interestingly, as more threads are used, performance worsens, echoing the results shown in the previous section.
This is due to the fact that when the number of threads is increased, synchonization between threads will increase whilst the potential number of corse which can be fused decreases.
In the case where the application does not feature highly parallel tasks, de-prioritising core compositions can negatively impact performance.
This signifies that for the benchmark \bench{Audiobeam}, it is more important to fuse cores than add more and more threads to the application.


\begin{figure}
  \includegraphics[width=1\textwidth]{streamit-paper/graphics/unrolled_fm.pdf}
  \caption{Distribution of FMRadio performance when modifying the amount of threads, composition and unrolling factor.}\label{fig:fmunroll}
\end{figure}

\subsection{Impact of Loop Unrolling}
In this section the impact compiler optimizations is studied by focusing on loop unrolling.
Filters containing large amounts of loops potentially contain high degrees of instruction level (ILP) and memory level parallelism.
When considering that the DMP uses the EDGE ISA, loop unrolling will increase the size of blocks, which facilitates the ability to extract ILP out of blocks.
Loop unrolling may also yield similar results to vectorization when vectorization may not easily be applied or available.

Figure~\ref{fig:fmunroll} presents an example of how loop unrolling affects performance on the \bench{FMRadio} benchmark.
The graph presents the same information as Figure~\ref{fig:audiototal} but with different executions of the benchmark when optimizing for speed and unroll factors 4, 16, and 64.
Figure~\ref{fig:fmunroll} shows that unrolling loops for \bench{FMRadio} can greatly improve performance by up to 8x compared to the non-unrolled version.
Another observation is that the best execution times for each of the threaded versions when unrolling does not follow the same trend seen in Figure~\ref{fig:threadtrend}.
The leftmost curve performance peaks at two threads whereas the rightmost peaks at five.
As increasing the unrolling factor reveals more ILP, core-composition can be used more efficiently.
Therefore, if the number of threads is increased, this reduces the potential number of cores that can be fused per thread.
This example therefore demonstrates that whilst the optimal number of threads is independent of the number of cores there still exists trade-offs between the two when including certain optimisations.
This signifies that the amount of resources available to each thread must be taken into consideration before generating the program to balance the trade off between ILP and TLP.

\begin{landscape}
\begin{figure}\hspace{-1em}
    \includegraphics[width=1\linewidth,keepaspectratio]{streamit-paper/graphics/threadcompbench.pdf}
    \caption{Speedup obtained by choosing best core composition, best
      thread number and the combination of both optimisations. The baseline for the speedup measurement is single core, single thread execution using O2 compiler optimisations. Higher
      is better.}\label{fig:overviewhist}
\end{figure}
\end{landscape}

\subsection{Co-Design Space Best Results}

\begin{table}[!htb]
% The FFT need are variable
  \small
 \begin{tabular} { | l | l | l | l | l | }
 \hline
   \cellcolor[gray]{0.7}Audiobeam&  \cellcolor[gray]{0.7} Beamformer& \cellcolor[gray]{0.7}Bitonic-Sort  &  \cellcolor[gray]{0.7} BubbleSort &  \cellcolor[gray]{0.7}  CFAR\\ \hline
  1 & 2 & 33 & 0 & 0 \\ \hline
   \cellcolor[gray]{0.7}ChannelVocoder &  \cellcolor[gray]{0.7} FFT&  \cellcolor[gray]{0.7}FFT3 &  \cellcolor[gray]{0.7} FFT6&  \cellcolor[gray]{0.7}FilterBank \\ \hline
  1 & 12 & 44 & 96 & 9 \\ \hline 
   \cellcolor[gray]{0.7}FIR &  \cellcolor[gray]{0.7} FMRadio &  \cellcolor[gray]{0.7} InsertionSort &  \cellcolor[gray]{0.7} Matmul-Block &  \cellcolor[gray]{0.7} RadixSort\\ \hline
  0 & 7 & 0 & 7 & 0 \\ \hline
 \end{tabular}
  \caption{Number of split joins present in each benchmark.}\label{tab:splitjoin}
\end{table}

This section  presents the results of the entire co-design space exploration.
Figure~\ref{fig:overviewhist} characterizes how much of a performance increase is obtainable using a baseline of executing the benchmark on a single thread and single core without unrolling.
For each benchmark, the \textit{THREAD} bar represents the maximal speedup obtained by dividing the program into threads without fusing cores.
The \textit{CORE} bar represents the best speedup when the benchmark is executed in a single thread and fuse cores.
\textit{BOTH} represents the best speedup obtained for each benchmark using a combination of \textit{THREAD} and \textit{CORE}.
Finally, for each benchmark, the results are obtained for both an unrolled and not unrolled version to compare how the compiler optimisation affects performance.
Figure~\ref{fig:overviewhist} shows that when loops are not unrolled, composing cores will not greatly improve performance.
This is due to the fact that the amount of ILP found in filters without the unrolling is too little for there to be any benefit of composing cores.

In the scenario where there are no specific optimisations for composition, multithreading will be the main source of performance.
This can be seen when studying the geometric mean,without unrolling.
Finding the correct number of threads gives a speedup of 1.92 compared to 1.33 when using only core composition, which is an improvement of 44\%.
This changes when taking unrolling into account as the core compositions can be used more efficiently.
In this case, the speedup obtained from only composing cores is only 13\% worse than using only threads.
For the \bench{FMRadio} benchmark, unrolling makes using only core-composition better than only using threads.
This information corroberates with the data seen in Figure~\ref{fig:fmunroll}; it presents a unique case where the effect of core composition is important enough to change the dominant performance enhancer.
Thus loop unrolling demonstrates that the StreamIt programs must be modified to take advantage of the core composition.

Overall the StreamIt programs demonstrate that multi-threading is the prevalent leader of performance, even with unrolling turned on.
This is natural as StreamIt applications are naturally geared towards TLP as most programs have at least one SplitJoin as seen in the Table~\ref{tab:splitjoin} which gives the number of split-joins per benchmark.
Benchmarks with SplitJoins will naturally benefit from splitting the program into threads~\cite{thiesStreamit2010}.
%Make sure this is 100% true but as far as I remember this is the case
Those that do not feature SplitJoins can still be parallelised by splitting a Pipeline into multiple parts.
For example, benchmark \bench{FIR} features no SplitJoins, yet splitting the Pipeline in 2 will result in a 1.40x speedup.
However, it is important to note that whilst finding the optimal thread mapping may result in higher performance improvements than finding the optimal composition for a single thread, the best performance is always obtained through a combination of both optimizations.
For cases such as \bench{BeamFormer} the optimal pairing results in a 1.8x speedup compared to simply finding the best multit-threaded version.
On average, the optimal combination leads to a 1.5x performance increase compared to only multithreading.

\subsection{Summary}
This section demonstrated that each parameter has a large effect on the performance of the workload.
Regardless of using core composition or not, there exists for each benchmark an optimal number of threads.
Unrolling is effective at exposing more opportunities for composition due to increased ILP but there is a balance to strike between extracting ILP and TLP.
Figure~\ref{fig:overviewhist} shows there is a 3x benefit (overall) by automating the partitioning of both the software (threads) and hardware (cores).

