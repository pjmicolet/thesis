This section describes the exploration of the software/hardware co-design space.
The software side includes partitioning the program, determining the number of threads and the loop unrolling compiler optimization.
The hardware side is about finding out the best core composition that maximizes performance for a given partitioning.

\subsection{Thread Partitioning}

This section first starts with analyzing the impact of thread partitioning on performance.
Thread partitioning is about deciding how many threads to create and how to partition StreamIt filters into these threads.
To simplify this study, the default streaming partitioner is used to decide on how to allocate filters to threads which is based on simulated annealing~\cite{simulatedAnnealing1983}.

On the hardware side, two scenarios are considered:
the ``without composition scenario'' where there is exactly one core per thread and the ``with composition scenario'' where each thread receives between 1 and 15 cores.

Figure~\ref{fig:threadtrend} shows how performance varies under both scenarios as a function of the number of threads.
Regardless of how cores are composed it can be observed that curves for a benchmark follow the same trend.
The optimal number of threads using core composition is very similar to the scenario without composition.
This important observation means that the optimal number of threads for a benchmark can be estimated independently from the hardware composition.
The system can therefore proceed in two stages: first determine the optimal number of threads and then decide on a core composition.

Figure~\ref{fig:threadtrend} also shows that the performance of most benchmarks starts to deteriorate passed a certain number of threads making it critical to not over-allocate threads.
This number of threads varies between benchmarks, thus it motivates the use of machine learning to decide the optimal number of threads to use.
Finally it is important to observe that executions without compositions always perform worse.
This demonstrates that composing cores is essential to obtain the best performance from a workload.


\subsection{Core Composition}

\begin{figure}[h]
  \includegraphics[width=1\textwidth]{streamit-paper/graphics/audiobeam_tots.pdf}
  \caption{Distribution of Audiobeam performance when modifying the amount of threads and compositions.}\label{fig:audiototal}
\end{figure}

Using core composition, the processor fuses a number of cores and associates them to a thread to increase single threaded performance.
Whilst this flexibility is advantageous, choosing the right amount of cores for a given thread is difficult due to the large number of possible configurations~\cite{gulati2008multitaskingdmc}.

Figure~\ref{fig:audiototal} shows how threading and core-compositions affect performance for the \bench{Audiobeam} benchmark.
The curves represent the density distribution for different core compositions as a function of the number of threads.
The right hand side Y-axis represents the number of threads present in the current version of the benchmark whilst the left Y-Axis represents the density normalized by the total number of points in the design space.
For each of the threaded versions the benchmark was ran using 100 different core-compositions.
The density curve for thread 15 is a single point as there exists only a single composition, so a line was drawn to represent where that point lies.

The variance of each of the curves represents the influence of composition on the benchmark's performance for a given number of threads.
For this benchmark the impact of core composition is actually very large for the best performing number of threads (1--5).
Interestingly, as more threads are used, performance worsens, echoing the results shown in the previous section.
This signifies that for the benchmark \bench{Audiobeam}, it is more important to fuse cores than add threads for the DMP.
Adding threads will incur an extra cost of synchronization by increasing the number of thread barriers.
Since fusing cores can improve the performance of serial execution, thread synchronization can be avoided. 
However as increasing the number of threads will reduce the amount of cores that can potentially be composed per thread, this reduces the potential speedup.
; which explains these results.
%By fusing cores, this can reduce the stress of having multiple threads whilst still getting performance from having more cores being used.

\begin{figure}
  \includegraphics[width=1\textwidth]{streamit-paper/graphics/unrolled_fm.pdf}
  \caption{Distribution of FMRadio performance when modifying the amount of threads, composition and unrolling factor.}\label{fig:fmunroll}
\end{figure}

\subsection{Impact of Loop Unrolling}
In this section the impact compiler optimizations is studied by focusing on loop unrolling.
Filters containing large amounts of loops potentially contain high degrees of instruction level (ILP) and memory level parallelism.
In the case of the EDGE ISA, loop unrolling will increase the size of blocks, which facilitates the ability to extract ILP out of blocks.
Loop unrolling may also yield similar results to vectorization when vectorization may not easily be applied or available.

Figure~\ref{fig:fmunroll} presents an example of how loop unrolling affects performance on the \bench{FMRadio} benchmark.
The graph presents the same information as Figure~\ref{fig:audiototal} but with different executions of the benchmark when optimizing for speed and unroll factors 4, 16, and 64.
Figure~\ref{fig:fmunroll} shows that unrolling loops for \bench{FMRadio} can greatly improve performance.

Another observation is that the best execution times for each of the threaded versions when unrolling does not follow the same trend seen in Figure~\ref{fig:threadtrend}.
The leftmost curve performance peaks at two threads whereas the rightmost peaks at five.
As increasing the unrolling factor reveals more ILP, core-composition can be used more efficiently.
Therefore, if the number of threads is increased, this reduces the potential number of cores that can be fused per thread.
This example therefore demonstrates that whilst the optimal number of threads is independent of the number of cores there still exists trade-offs between the two when including certain optimisations.
This signifies that the amount of resources available to each thread must be taken into consideration before generating the program to balance the trade off between ILP and TLP.

\begin{sidewaysfigure}
 \centering
  \centering
    \includegraphics[width=1\textwidth]{streamit-paper/graphics/threadcompbench.pdf}
    \caption{Speedup obtained by choosing best core composition, best
      thread number and the combination of both optimisations. The baseline for the speedup measurement is single core, single thread execution using O2 compiler optimisations. Higher
      is better.}\label{fig:overviewhist}
\end{sidewaysfigure}

\subsection{Co-Design Space Best Results}

\begin{table}[!htb]
% The FFT need are variable
  \small
 \begin{tabular} { | l | l | l | l | l | }
 \hline
   \cellcolor[gray]{0.7}Audiobeam&  \cellcolor[gray]{0.7} Beamformer& \cellcolor[gray]{0.7}Bitonic-Sort  &  \cellcolor[gray]{0.7} BubbleSort &  \cellcolor[gray]{0.7}  CFAR\\ \hline
  1 & 2 & 33 & 0 & 0 \\ \hline
   \cellcolor[gray]{0.7}ChannelVocoder &  \cellcolor[gray]{0.7} FFT&  \cellcolor[gray]{0.7}FFT3 &  \cellcolor[gray]{0.7} FFT6&  \cellcolor[gray]{0.7}FilterBank \\ \hline
  1 & 12 & 44 & 96 & 9 \\ \hline 
   \cellcolor[gray]{0.7}FIR &  \cellcolor[gray]{0.7} FMRadio &  \cellcolor[gray]{0.7} InsertionSort &  \cellcolor[gray]{0.7} Matmul-Block &  \cellcolor[gray]{0.7} RadixSort\\ \hline
  0 & 7 & 0 & 7 & 0 \\ \hline
 \end{tabular}
  \caption{Number of split joins present in each benchmark.}\label{tab:splitjoin}
\end{table}

This section  presents the results of the entire co-design space exploration.
Figure~\ref{fig:overviewhist} characterizes how much of a performance increase is obtainable using a baseline of executing the benchmark on a single thread and single core without unrolling.
For each benchmark, the \textit{THREAD} bar represents the maximal speedup obtained by dividing the program into threads without fusing cores.
The \textit{CORE} bar represents the best speedup when the benchmark is executed in a single thread and fuse cores.
\textit{BOTH} represents the best speedup obtained for each benchmark using a combination of \textit{THREAD} and \textit{CORE}.
Finally, for each benchmark, these results were obtained for both an unrolled and not unrolled to compare how unrolling affects performance.
Figure~\ref{fig:overviewhist} shows that when loops are not unrolled, composing cores will not greatly improve performance.
This is due to the fact that the amount of ILP found in filters, without the unrolling, is too little for there to be any benefit or composing cores.

When studying the geometric mean, without unrolling, finding the correct number of threads gives a speedup of 1.92 compared to 1.33 when using only core composition.
This changes when taking unrolling into account as the core compositions can be used more efficiently.
In this case, the speedup obtained from only composing cores is only 13\% worse than using only threads. 
The unrolling demonstrates that the StreamIt programs must be modified to take advantage of the core composition.
Overall, in the StreamIt programs, multi-threading is the prevalent leader of performance.
This is natural as StreamIt applications are naturally geared towards TLP as most programs have at least one SplitJoin as seen in the Table~\ref{tab:splitjoin} which gives the number of split-joins per benchmark.
Benchmarks with SplitJoins will naturally benefit from splitting the program into threads~\cite{thiesStreamit2010}.
However, it is important to note that whilst finding the optimal thread mapping is better than the best composition, the best performance is always obtained through a combination of both optimizations. 

\subsection{Summary}

This section demonstrated that each parameter has a large effect on the performance of the workload.
Regardless of using core composition or not, there exists for each benchmark an optimal number of threads.
Unrolling is effective at exposing more opportunities for composition due to increased ILP but there is a balance to strike between extracting ILP and TLP.
Figure~\ref{fig:overviewhist} shows there is a 3x benefit (overall) by automating the partitioning of both the software (threads) and hardware (cores).

