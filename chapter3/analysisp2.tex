This section evaluates a real implementation of a value predictor, using the block based D-VTAGE predictor.
Before conducting the performance analysis, it is important to understand how the predictor must be configured.
This section therefore starts with a block analysis of the SD-VBS benchmarks.

\subsection{Block analysis}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{chapter3/graphics/joint.pdf}

    \caption{Average number of register reads and writes per EDGE block and the number of unique blocks comprising different percentages of the total execution (in blocks) of each of the benchmarks.}
    \label{fig:edge_reg_read}
	\vspace{1em}
\end{figure}

In a block based D-VTAGE predictor a single prediction request fetches multiple values.
Due to the fact that the block size is fixed ahead of time~\cite{peraisBeBop2015}, determining the correct size is important.
Having a block with many values means that the D-VTAGE predictor can predict a higher number of register reads for a single EDGE block, however this comes at the expense of having fewer blocks in the predictor tables.
On the other hand, a small block size allows to have multiple blocks stored at a time but means that not all instructions can have their values predicted in the EDGE block.
This could be potentially remediated by allowing a single EDGE block to occupy multiple predictor blocks, however this technique is not explored in this thesis.


To determine the size, all the EDGE blocks of the SD-VBS benchmarks are analysed to determine the average unique register read and write count per blocks throughout the execution of the benchmarks.
The writes are tracked as it provides information on the potential number of register dependencies between blocks found in each of the applications.
The left hand side of figure~\ref{fig:edge_reg_read} shows the average number of register read and writes for each of the benchmarks.
Whilst \bm{Disparity} \bm{Localization}, \bm{Texture\_Synthesis} and \bm{Tracking} have higher register read counts than the average, most of them only have 5 writes per block.
This means that having a D-VTAGE block size of 5 could capture all dependencies; however this would require detection of which reads are data-dependent, which is beyond the scope of this thesis.
As Section~\ref{chp:chp3:sec:analysis} showed these benchmarks benefit from value prediction, to ensure that most blocks have all their register reads captured, a block size of at least 8 is required.

\paragraph*{Block variation analysis}

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=1\textwidth]{chapter3/graphics/unique_blocks.pdf}%

%    \caption{Number of unique blocks comprising different percentages of the total execution (in blocks) of each of the benchmarks.}
%    \label{fig:totblock}
%	\vspace{1em}
%\end{figure}

One method of understanding how the block based D-VTAGE predictor will perform is to study the number of unique blocks found in each of the benchmarks.
Benchmarks that feature a smaller number of unique blocks can potentially benefit more from value prediction as the predictor cannot hold many blocks at a time.
Reporting all unique blocks executed is not a proper evaluation of the variety of blocks found in the benchmark, as some will most certainly be executed more times than others.
To account for this, blocks are only counter towards the unique block counter as long as they represent a certain percentage of the total number of blocks executed.

The right hand side of figure~\ref{fig:edge_reg_read} shows the number of unique blocks found in each benchmark that account for 50\%,80\%,90\% and 99\% of the total number of executed blocks.
As can be seen, applications \bm{Disparity}, \bm{Multi\_NCut} and \bm{Sift} \bm{Stitch} and \bm{Tracking} execute fewer than 50 unique blocks during 90\% of its total execution.
This is promising as it means that there is a high chance that the predictor requires fewer entries to capture all possible blocks in the application.
On the other hand, \bm{Localization}, and \bm{SVM} execute over 100 blocks throughout 90\% of their execution, twice as many as the previously mentioned benchmarks.
For these applications, it might be harder to predict values as new blocks may overwrite entries in the predictor.

\vspace{-1em}
\subsection{Setup}

\begin{table}[t]
\smaller
\centering
\begin{tabular}{p{5.2cm} p{3cm}}
\toprule
\textbf{Parameter} & \textbf{Values} \\ \midrule
\# Base Entry & 1536\\
\# Tagged & $6\times1536$\\
\# Block in Spec Window & 4 \\ \hline \midrule
\# of values per entry & 8, 16\\
Confidence Value & 4 or 7 with FPC \\ \bottomrule
\end{tabular}
  \caption{D-VTAGE table configuration and configurable parameters.}\label{tab:vtage-conf}
\vspace{1em}
\end{table}

This section demonstrates how the block based D-VTAGE value predictor improves the performance of core composition using the round-robin fetch scheme (RRF).
Serial fetch is not explored since the previous section showed that when using value prediction, RRF always provides the best results.
The results reported in this section represent the culmination of hardware modifications for core composition discussed in this chapter.
All benchmarks are executed using a 16 core composition.

\paragraph*{Predictor size}
The D-VTAGE size configuration can be found in Table~\ref{tab:vtage-conf}.
The Last-Value Table shares the same number of values as the Base Entry and uses a 5-bit tag.
Each tagged entry has a tag of varying size (first table is 12 bits, second 13 bits so on and so forth).
The total number of values for the D-VTAGE predictor is taken from Perais et al's work ~\cite{peraisBeBop2015}.
The adopted size for this chapter is the medium predictor as it provides a good compromise between performance and total size of the predictor.
The blocks in the speculative window represents the total number of in-flight blocks that can be handled by the speculative window, which is 4.

\paragraph*{On area and power/energy consumption}
Since value prediction is still an experimental piece of hardware, this chapter focuses on demonstrating that they are useful in the context of core composition.
This chapter is therefore a limit study of the performance improvements obtained via value predictors in large core compositions.
This study motivates the necessity of value predictors, to push further research into how to efficiently implement them in the hardware.
Therefore, no energy/power consumption or area information is provided here.

\vspace{-0.5em}
\paragraph*{Parameter tuning}
Two features of the predictor can be modified: the number of values per entry and at what confidence a prediction is used.
Modifying the required confidence explores the trade-off between high coverage and low misprediction.
The original D-VTAGE uses Forward Probabilistic Counters (FPC)~\cite{riley2006fpc} to increment the confidence, and only used predictions once the counter was set to 7.
This confidence rate ensured that the predictor had an accuracy of over 99\%, but at the expense of a low coverage, 20\% on average~\cite{peraisBeBop2015}.
The confidence of 4 is chosen as it allows for very fast deployment of predictions, whilst still ensuring that the values have been trained at least a few times.
The FPC vector used in this Chapter is the same as the one in Perais et al.'s work $\{1,\frac{1}{16},\frac{1}{16},\frac{1}{16},\frac{1}{16},\frac{1}{16},\frac{1}{32},\frac{1}{32}\}$.
\vspace{-0.5em}
\paragraph*{Block size}
Whilst most applications only have 5 reads per block, \bm{Disparity} and \bm{Localization} have at least 8 reads and they both benefit from value prediction as seen in Section~\ref{chp:chp3:sec:analysis}.
Since there is no method of determining which reads may have data-dependencies, it is important to have a block size that captures the largest register read average for the set of benchmarks which is 8.
This section explores block sizes of 8 and 16 to ensure that all register reads are potentially covered. 
The block size of 16 is explored to see how capturing a higher number than the average number of reads affects the performance of the predictor. 

\paragraph*{Prediction Delay}
In the original D-VTAGE proposal, they suggest that the predictor can generate \textit{x} values per cycle where \textit{x} is the issue width of the core.
This means that 8 values can be generated per cycle, as this is the issue width of a core (see Chapter~\ref{chp:setup} Section~\ref{chp:setup:conf}).
Since a request to the value predictor can be made at the same time as the start of a block fetch as seen in Figure~\ref{fig:bad_overview}, this gives enough time to fetch all the values before a block is fully dispatched as most blocks have at least 8 instructions.


\subsection{Results}

\paragraph*{Performance}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{chapter3/graphics/vtage_speed3.pdf}
    \caption{Speedup obtained using a D-VTAGE value predictor and RRF with 16 cores composed. Baseline is 16 cores composed with SF and without value prediction. Both use perfect branch prediction. Higher is better.}
    \label{fig:vtage_perf}
	\vspace{1em}
\end{figure}

Figure~\ref{fig:vtage_perf} reports the speedup obtained with the different configurations of the D-VTAGE predictor with RRF.
The baseline is a 16 core composition that uses the serial fetching (SF) scheme without any value prediction and using perfect branch prediction.
To better understand the performance of the predictors, each configuration is compared to the perfect predictor, which can predict every register value at any instance.
Throughout this section, the configurations of D-VTAGE are labelled: \textbf{D-VTAGE\_\textit{Confidence}\_\textit{BlockSize}}.

On average, using a D-VTAGE value predictor with the RRF scheme results in an average speedup of 1.32x.
When comparing the performance between the different configurations, the main observation is that using a higher confidence results in better performance, however slight, 1.35x for D-VTAGE\_7\_16 compared to 1.31x for D-VTAGE\_4\_16.
However, for the benchmark \bm{Disparity} using a higher confidence leads to less of a performance increase 1.75x speedup compared to 2.0x for a confidence counter of 4.
This is due to the fact that \bm{Disparity} is composed of loops with highly predictable values, and thus deploying predictions earlier results in better performance improvements without risking a higher misprediction rate.
On the other hand, \bm{Localization} and \bm{MSER} perform worse with a lower confidence counter, and result in a slowdown.
Benchmarks \bm{Sift} and \bm{Tracking} perform better with a higher confidence, but the results of using a 4 bit counter never negatively impact performance.

The 16 values per block does not lead to better performance than 8 values but stays on par with it.
The only noticeable difference is with \bm{MSER} where the smaller block performs better.
A lower number of values per block allows the predictor to capture a higher variety of executed blocks.
Figure~\ref{fig:edge_reg_read} shows that \bm{MSER} has at least 50 different blocks throughout 99\% of its execution, and on average, only 3 register reads per block.
Therefore, for \bm{MSER}, a predictor with fewer values per entry can capture more of the variation.
On the other hand, for \bm{Disparity}, 16 values per entry is required to get the best performance.
This once again corroborates with the information from Figure~\ref{fig:edge_reg_read}.
\bm{Disparity} has a higher average of reads per block, but features a very low number of unique blocks.
Therefore having a larger entry in the table captures more values, but does not suffer from having entries being replaced by new blocks.

Comparing the performance of the different configurations to the perfect value predictor, it is apparent that most benchmarks do not achieve their maximum potential as the perfect predictor results in an average speedup of 1.88x compared to 1.32x of D-VTAGE.
However, benchmarks \bm{Disparity} and \bm{Multi\_NCut} show how value prediction with RRF is a promising lead for improving the performance of core composition.

\paragraph*{Coverage and Accuracy}
To better understand the performance of the different D-VTAGE configurations, the predictors' coverage and misprediction rates are recorded.
The coverage is measured by comparing the number of correct register read values to the total number of executed register reads.
Since EDGE is a block based architecture, it is important to study the coverage and accuracy on both a per-instruction level and per-block level.
This is due to the fact that a single read misprediction leads to flushing the block and all younger blocks in the chain.
Also a single accurate register prediction may not necessarily improve performance as other un-predicted registers in the block may be data-dependent with other blocks, reducing ILP.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{chapter3/graphics/coverageFull2.pdf}
    \caption{Prediction coverage at a block and instruction level. Higher is better}
    \label{fig:vtag_cov_block}
	\vspace{1em}
    \centering
    \includegraphics[width=1\textwidth]{chapter3/graphics/predAcc2.pdf}
    \caption{Accuracy of the different D-VTAGE predictors at a block and instruction level. Higher is better.}
    \label{fig:vtag_accuracy_block}
	\vspace{1em}
\end{figure}
In this chapter, only blocks that are committed count towards the predicted blocks as any flush will artificially inflate the coverage count.
Figure~\ref{fig:vtag_cov_block} displays the number of blocks which have at least one prediction, relative to the total number of committed blocks and the number of predicted register reads relative to the total executed register reads.
The figure shows similar patterns for both coverages: the predictors with lower confidence counters have a higher coverage, 65\% compared to 31\% for the high confidence counter.
This is normal: lower confidence allows predictions to be used earlier, which will increase coverage.

Whilst the block coverage is high, the coverage for registers reveals that not all registers are being predicted.
Once again, lower confidence equates to higher coverage, however this time it's 30\% for a confidence of 4 and under 25\% for a confidence of 7 with FPC.
The register coverage for a confidence of 7 is in line with the coverage reported in Perais. et al's work ~\cite{peraisBeBop2015, peraisVTAGE2014} if not slightly higher.
The coverage may be slightly higher due to the fact that the values being predicted are often either loop increments or memory increments which can easily be predicted.

The register coverage helps explain why the high block coverage does not lead to better performance: whilst most blocks may have a valid prediction, some of the register reads cannot be predicted.
This may be due to data being carried over which is unpredictable; for instance a loop which sums all values of an array into a single integer will pass that integer via a register.
This register will cause a data dependency between loop iterations and will be difficult to predict.


\bm{Localization}, \bm{MSER}, \bm{SVM} and \bm{Texture\_Synthesis} often have much lower coverage than the rest of the programs.
Recalling Figure~\ref{fig:edge_reg_read} which shows the number of unique blocks executed throughout the programs, these benchmarks had a higher count of unique blocks and thus a higher chance of encountering a new block.
As blocks require multiple executions to train the predictor (if the values in the register are indeed predictable), than the higher diversity of blocks makes it harder.
Thus, these benchmarks will naturally have a harder time to benefit from value prediction.

Figure~\ref{fig:vtag_cov_block} shows that a lower confidence allows for higher coverage, yet the speedups seen in Figure~\ref{fig:vtage_perf} indicate that a higher confidence leads to better performance.
To confirm this, Figure~\ref{fig:vtag_accuracy_block} presents the prediction accuracy rate for each benchmark at a per-block and per-instruction base respectively.
The D-VTAGE predictor maintains a 99\% accuracy, whether at the block level or instruction level.
However, for \bm{Localization}, \bm{MSER}, \bm{SVM}, \bm{Texture\_Synthesis} and \bm{Tracking}, the block level accuracy for a confidence of 4 is under 98\%, and even down to 93\% for \bm{MSER}.
The effect of a value misprediction is similar to a branch misprediction: it causes a flush of all blocks younger than the block with the incorrect misprediction.
Just like branch prediction, large core compositions are very sensitive to mispredictions, thus an accuracy rate of 93\% will impact performance if the blocks are small.
This explains why the low confidence counter sometimes performs worse than the higher confidence as it mispredicts more often.
\vspace{-1em}

\subsection{Performance with non-perfect branch prediction}

\begin{table}[t]
  \smaller
  \centering
 \begin{tabular} { | l | l | l | l | l | }
 \hline
   \cellcolor[gray]{0.7}Disparity & \cellcolor[gray]{0.7} Localization& \cellcolor[gray]{0.7} MSER& \cellcolor[gray]{0.7} Multi\_NCut& \cellcolor[gray]{0.7} Sift\\ \hline
	98\%  & 95\% & 85\%  & 100\%& 99\%\\ \hline
	 \cellcolor[gray]{0.7} Stitch & \cellcolor[gray]{0.7} SVM & \cellcolor[gray]{0.7} Text. Synth & \cellcolor[gray]{0.7} Tracking&\\ \hline
	  95\%& 93\%& 98\%& 98 \%&\\ \hline
	\end{tabular}
  \caption{Branch prediction accuracy in percentage for each of the benchmarks.}\label{tab:sd-vbsbpred2}
  \vspace{1em}
\end{table}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{chapter3/graphics/with_bpred.pdf}

    \caption{Speedup obtained using a D-VTAGE value predictor and RRF with 16 cores composed. Baseline is 16 cores composed with SF and without value prediction. Both use a non-perfect branch predictor. Higher is better.}
	\vspace{1em}
    \label{fig:bpred}
\end{figure}
Finally, this section studies how non-perfect branch prediction affects the performance of the applications with RRF and value prediction.
To conduct this analysis, the perfect branch predictor was modified to randomly mis-predict given a certain accuracy requirement.
For each of the benchmarks, the branch prediction accuracies recorded in Chapter~\ref{chp:cases} are used.
The modified branch predictor is used to ensure that the same accuracies are maintained, even with the potential influence of a value predictor.
These accuracies can be found in Table~\ref{tab:sd-vbsbpred2}.

Figure~\ref{fig:bpred} reports the speedup obtained with the different configurations using a non-perfect branch predictor.
In this scenario, the baseline is a 16 core composition with SF and no value prediction, using the branch prediction accuracies defined in Table~\ref{tab:sd-vbsbpred2}.
The figure shows an extra configuration that shows the results of the perfect value prediction with perfect branch prediction, to give perspective of how branch prediction affects performance.
As the figure shows, the new configuration still performs well, due to the fact that most benchmarks had either high enough branch prediction accuracies, or blocks large enough to sustain 16 composed cores.
This confirms that even without perfect branch and value prediction, performance can be improved by up to 2.8x, with an average of 1.30x.
This is to be expected as branch prediction has a long history of research whilst value prediction has only recently seen a resurgence in interest.
The results encourage more development in value prediction, as the perfect value predictors show there is still a lot of performance to be gained.


%\subsubsection{Size of stride}

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=1\textwidth]{chapter3/graphics/strides.pdf}
%    \caption{Distribution of the size of strides for each benchmark.}
%    \label{fig:strides}%
%	\vspace{1em}
%\end{figure}

%Whilst 64 bit strides are used throughout the experiments to capture all the potential performance, 64 bits per entries may not be necessary.
%Figure~\ref{fig:strides} shows the average number of valid predictions whose strides could fit in either an 8, 16, 32 or 64 bit signed integer for each of the benchmarks.
%The data was taken from the D-VTAGE configuration that led to the highest performance overall for each of the benchmarks.
%In Perais' et al'.s work they recommend that a medium-sized D-VTAGE predictor use 8-bit strides.
%Overall, Figure~\ref{fig:strides} shows that using 8 bit strides should be sufficient for most benchmarks, however \bm{Disparity}, one of the benchmarks which benefits most from value prediction, would lose almost 25\% of its coverage.
%Therefore, for these set of applications, a stride of at least 16 bits is necessary, as it captures 91\% of the total strides found in all the benchmarks.

%\subsection{Putting it all together}
%This section has demonstrated that a real value predictor can sometimes perform as well as a perfect predictor, resulting in a speedup of up to 2.7x for \bm{MSER} and 2x \bm{Disparity}, when paired with the round robin fetching scheme.
%The section also showed that whilst a lower confidence rate does increase the coverage, it does lead to a higher misprediction.
%As core composition is sensitive to any kind of misprediction that causes a flush, the predictor with confidence value 4 performs less well than confidence value 7 with FPC.
%Finally, the analysis of stride size shows that for this set of benchmarks, a 16 bit stride is sufficient to capture 91\% of the strides for the benchmarks.
%Using this information, a recommended D-VTAGE configuration for the dynamic multicore processor would take up 39kB of memory, which is slightly larger than an L1 cache.
