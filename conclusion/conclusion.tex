\chapter{Conclusion}~\label{chp:conclusion}
This thesis has explored how static ahead of time reconfiguration, dynamic runtime adaptation and microarchitectural modifications can make dynamic multicore processors (DMP) more practical.
Chapter~\ref{chp:streamit} showed that a multithreaded application can automatically be mapped ahead of time to a DMP, where different number of cores are composed for each thread.
Chapter~\ref{chp:cases} demonstrated that at runtime the DMP can automatically change the size of a composition to ensure that the system is maximising speed whilst being energy efficient.
Both chapters extract features from the software and use machine learning techniques such as linear regression and k Nearest Neighbors to determine correct configurations.
Using cross-validation, the two chapters show that the automatic reconfiguration can be used in new unseen situations.
The chapters also covered how source-level code optimisations can help improve the performance of core composition when access to the compiler is not available.
Finally, Chapter~\ref{chp:hardchanges} explored how modifying the fetching scheme in core composition and adding value prediction can improve the performance of large core compositions.

This chapter summarises the contributions of the thesis, followed by a critical analysis of chapters ~\ref{chp:streamit}, \ref{chp:cases} and \ref{chp:hardchanges}.
The final section covers future work in core composition.

\section{Contributions}
This section summarises the contributions of this thesis, specifically the work conducted in chapters  ~\ref{chp:streamit}, \ref{chp:cases} and \ref{chp:hardchanges}.
\subsection{Static ahead of time thread and core partitioning}

Chapter~\ref{chp:streamit} tackled mapping streaming applications to a DMP.
This process involved determining the best number of threads for an application and how many cores need to be composed per thread.
The chapter first presented a design space exploration analysis of a set of streaming applications where 1500 different thread to core composition mappings were used.
A compiler optimisation was also explored, loop unrolling, and showed that core composition is sensitive to block size: the larger the blocks the more likely core composition is going to more useful.
Overall the design space exploration underlined that in order to get the best performance, a mix of multi-threading and core composition is required and leads to an average speedup of 3x compared to a single core single thread.
 
This was followed by the presentation of two models that can determine the number of threads that lead to the best performance for a given application and the number of cores per thread.
The thread model used k Nearest Neighbor to classify a program based on its structure.
The core composition model uses linear regression to determine how large a composition must be by analysing the average size of an unconditional block of operations found in the thread.
Using leave one out cross-validation, automated model leads to DMP configurations that are within 16\% of the best from the total exploration space.

This proved that a machine learning model can be used to determine good configurations ahead of time by only analysing static features of an application.
This means that if obtaining the fastest execution time is important, this thesis showed that this can be achieved automatically for DMPs without having to search the exhaustive space.

\subsection{Dynamic runtime adaptation for efficient execution}

Chapter~\ref{chp:cases} first covered how branch prediction and synchronisation costs affect the performance of different core composition sizes.
It confirmed the early analysis of Chapter~\ref{chp:streamit} that large EDGE blocks are critical to the efficient use of large core compositions: it reduces the branch prediction accuracy requirements and reduces the cost of synchronising cores.
Then, an in depth comparison of dynamic and static ahead of time core compositions was conducted on a set of vision benchmarks.
This study showed that whilst dynamic core compositions do not outperform static ahead of time compositions in terms of speed, dynamically changing the size of a composition can help reduce energy consumption.
By allowing the DMP to switch between compositions of sizes 1, 2, 4, 8 and 16 cores, dynamic adaptation can reduce energy consumption by 42\% on average compared to a static ahead of time configuration.

The chapter then studied how the latency caused by switching the size of the composition can affect the energy savings.
It was determined that, as phases are long, on average 100k cycles, the reconfiguration penalty can be between 100 to 1000 cycles without affecting savings.
Finally, a simple linear regression was used to determine when to switch the size of a core composition at runtime.
The model analysed the instruction mix of the blocks being executed to determine if the current composition was adequate.
Using this automated adaptation scheme led to an average energy saving of 36\% which was close to the best possible results.

This chapter underlined that a DMP can dynamically be reconfigured to reduce energy consumption at runtime without the need of profiling information for each application.
This allows programmers to be able to immediately benefit from the DMP's flexibility without having to concern themselves with the added work of profiling their programs.


\subsection{Adapting hardware to improve core composition performance}
Chapter~\ref{chp:hardchanges} proposed two modifications to the DMP in order to improve performance of core composition.
The two modifications aim to reduce data-dependencies between cores in a composition and also increase the percentage of time each core in a large composition was active.
First, it analysed how the current fetching mechanism was not adequate for large core compositions when blocks are small.
As the current fetching mechanism focuses on filling the instruction window of a single core, many cores in the composition are left idle.
The chapter suggests a decentralised round-robin fetch scheme where cores fetch blocks out of order and dispatch them in order.

Second, the use of value prediction was motivated to reduce the penalty incurred by inter-block data dependencies.
The chapter suggests that a block based computational value predictor be used as it allows multiple predictions to be generated quickly.
This is followed by a performance analysis using a perfect value predictor with and without the round robin fetch scheme on the same benchmarks used in Chapter~\ref{chp:cases}.
The analysis shows that without value prediction, the round robin fetch scheme cannot improve performance due to the data dependencies found between blocks.
However, when both value prediction and the round robin fetching scheme are used, this can improve the performance of a 16 core composition by 1.8x on average.
Finally, a block-based Differential VTAGE (D-VTAGE) value predictor was implemented and its performance analysed.
Overall, using current state of the art value prediction with the round robin fetching scheme resulted in an average speedup of 1.33x, and could provide a speedup of up to 2.7x.

The results motivate more work in value prediction for core composition as it is an effective way to improve performance.
It also demonstrates that more research must be conducted on how core composition should behave, as the simplest mechanisms may not be the most effective ones.

\section{Critical Analysis}
%This thesis aimed to make dynamic multicore processors more practical, however it is important to conduct a critical analysis to understand the limitations of the research conducted.

\subsection{Simulation}

The experiments done in this thesis use a cycle-accurate simulator for the EDGE architecture.
This is due to the fact that there exists no processor that uses the EDGE architecture and also has core composition.
In fact, there exist no physical processor that implement core composition, thus simulation is the only current method of evaluating it.
This is also an issue for value prediction, as no processor manufacturer has yet to implement one.

Whilst simulation can provide a good overview of how can be affected by different configurations of the processor, runtime adaptation, and the effect of value prediction, certain inaccuracies may arise from the implementation of the model.
Furthermore, energy or power consumption can only be estimated through the use of a model.
A Register-transfer level (RTL) implementation of the EDGE processor that features core composition and value prediction would allow for better precision.

\subsection{Processor configuration}
The design exploration and results produced in this thesis all used the same core configuration.
Naturally, this means that some of the deductions are specific to the processor that was used during the experiments.
The thesis does provide a solid methodology for creating models that can predict a good configuration for any dynamic multicore processors but it cannot provide absolute truths.
For example, if cores can only fetch a single block at any given time, then composing cores would most likely improve performance at a faster rate, since the performance of a single core would be lower.
This means that the analysis of what loop optimisations make software run faster on compositions is potentially tied to the configuration of the cores.
Thus, whilst the results presented throughout this thesis demonstrate that DMPs can be made more practical, it does not provide information on when core composition should be considered a feature of the processor.

\subsection{Compiler}
In chapters ~\ref{chp:streamit}, \ref{chp:cases} one of the factors that limits performance in core composition is block size.
The chapters explored source-level transformations to improve the size of blocks.
However code transformations at the compiler level could potentially increase block sizes for some programs that could not be improved manually.
This could potentially affect the amount of instruction level parallelism found in some of the benchmarks explored throughout this thesis, which, naturally could affect energy savings when considering dynamic reconfiguration.

Furthermore hyperblock formation is currently limited to the merging of two blocks, which once again limits the potential size of a block.
Hyperblock formation also does not take into account any profiling information which could potentially impact the formation of blocks.
For example, the merging of two blocks that form an if/else statement is only useful if both conditions happen fairly regularly.
If this is not the case and one of the statements is taken most of the time then the block's size is only artificially increased, as some of the predicated instructions will never fire.


\section{Future Work}

Source level transformations were shown to help improve the performance of core composition, however they do not encompass all possible optimisations that can be applied to the code.
Exploring different compiler level optimisations could potentially help make core compositions even more effective.
For example, loops that feature a high amount of control flow currently cannot be unrolled as that still generates small blocks, as discussed in chapters ~\ref{chp:streamit} and ~\ref{chp:cases}.
However, employing optimisation techniques such as modulo scheduling can potentially help improve instruction level parallelism for such loops.
As for hyperblock formation, using execution traces to determine which blocks should be merged together can help increase the usefulness of hyperblocks.

On the hardware side, it is important to evaluate when core composition should be implemented in a processor.
Currently, the focus is on demonstrating that composing cores can lead to performance improvements, however there is no research being conducted on the type of cores that can benefit the most from composition.
Exploring how core composition can speedup the performance of processors with different core configurations can help shed light as to when core composition should be implemented.
This should be done by conducting a micro-architectural design space exploration where structures such as load-store queues, number of lanes a core has and cache sizes are modified.

Finally, pairing core composition with speculative parallel execution could provide an interesting avenue of research.
For example, programs that feature irregular paralellism have phases that alternate between embarrassingly parallel work, and highly serialised work.
A processor could begin my attempting to extract as much parallelism as possible via speculative parallelism, and adapt itself via core composition if the workload does not feature any parallelism.
This could help improve the performance of graph algorithms that often feature phases of parallel and serial work.
