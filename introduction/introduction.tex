\chapter{Introduction}
%From StreamIt paper
Multi-core processors are now common in all computing systems ranging from mobile devices to data centres.
As advances in single-threaded performance have slowed, multi-core processors have offered a way to use the increasing numbers of transistors available.
Due to the difficulty of designing more powerful cores, a shift towards adding more cores into a single package seems inevitable.
These high core-count processors are a subset of multi-core processors known as tiled architectures.
A tiled architecture such as Tilera~\cite{bell2008tile} or Raw~\cite{waingold1997raw} is composed of smaller simpler cores that are placed on a regular grid.
This improves hardware scalability and enables multi-threaded applications to exploit the large core count.

Yet, workloads that require high single-threaded performance are penalised by the simple nature of each core~\cite{eyerman2010amdahl}.
One solution to this problem is heterogeneous multi-cores which utilise cores with different levels of power and performance.
Although heterogeneous multi-cores are common place in mobile devices, they have little reconfiguration or adaptive capabilities (\eg only two type of cores available for ARM big.LITTLE).
Dynamic multi-core processors (DMPs) offer a solution to this problem by allowing cores to compose (or fuse) together~\cite{ipek2007CoreFusion} into larger compositions to accelerate single threads.
This produces ``on-demand'' heterogeneity where cores are grouped to adapt to the workload's demand.

Whilst the flexibility of a DMP is an advantage, it increases the complexity of obtaining the optimal performance out of applications.
If reconfiguration cannot be automated, then it is up to the programmer to decide how the processor must be partitioned, adding extra development work.
In order to motivate further development of DMPs, it is important to prove that they can be practical.
This thesis demonstrates that different techniques can improve the usability and performance of DMPs, making them an attractive approach towards processor design. 

\section{The Problem}
This section covers the different problems that are considered a limiting factor of DMPs in terms of their practicality.
\paragraph*{Reconfiguration}
Whilst there exists a multitude of proposed dynamic multi-core processor architectures as seen in Mittal's survey~\cite{MittalSurv2016}; work on understanding when to compose cores, or what type of programs can benefit the most out of core-composition is scarce.
A 16 core DMP for example has over 32,000 configurations when executing multi-threaded programs, making exhaustive search of the space impractical.
Therefore, without some method of automating the reconfiguration of the processor the programmer must have intimate knowledge of both the architecture and the programs that will execute on them in order to determine a good configuration.

Previous work on determining how many cores must be composed for a given program focus on using profiling information~\cite{pricopiSchedCoreComp2014} or heuristics based on observations~\cite{gulati2008multitaskingdmc}.
They consider core-composition to be a \textit{black box}: instead of trying to understand what features of a program lead to good performance, they evaluate it on different core-composition sizes and determine the best one.
The best configuration can be the one that leads to the fastest execution time, or the most efficient one based on a specific metric such as energy/power efficiency.
This approach makes DMPs less practical as it increases the amount of work required to ensure that workloads benefit from core-composition.

If a configuration of the processor that fits the user's requirements could automatically be determined without requiring multiple executions of the program at hand, then this would make the process of getting the best performance lightweight.
This would allow programmers to modify their programs without having to extensively re-profile their applications, making dynamic multi-core processors more approachable.
There are two possible methods of achieving this: either by having the hardware reconfigure itself automatically by analysing the executing program, or by having the compiler determine the configuration via an analysis of the program.
Using the hardware allows for greater flexibility as legacy software can immediately benefit from the re-configuration without having to be re-compiled.

\paragraph*{Maximising performance through software optimisations}
Not all applications benefit from executing on a core-composition, reducing the attractiveness of implementing the feature in a processor.
The lack of performance may be due to the fact that the programs are not designed to be executed on a system that supports core-composition.
Programmers may have to re-write their code to ensure that a program that currently does not perform any better could not be re-written to benefit from the new hardware.

However, there exists no information as to what optimisations may help improve the performance of applications on a dynamic multi-core processor.
Furthermore, a programmer may not necessarily have access to a compiler that provides passes that are targeted towards such systems.
In this case, it is important to explore source-level optimisations and study how they can help increase the performance of programs on core-composition.
By underlining which optimisations will lead to performance improvements, this encourages the ease-of-use of dynamic multi-core processors.

\paragraph*{Core composition mechanisms} 
Finally, as the concept of dynamic multi-core processors is still relatively new compared to other processor designs, the currently proposed techniques may not be enough to maximise performance.
For example, core-composition exploits instruction level parallelism (ILP) by executing a single thread on multiple cores.
Unfortunately unresolved data-dependencies, that often arise when many instructions can be executed in parallel, will reduce the potential performance improvements as instructions must wait on each other to execute.
This means that DMPs will not improve performance to the fullest of their capacities.

Previous work attempts to resolve this problem by applying register forwarding to try and reduce the latency caused by data-dependencies~\cite{robatmili2011uniproc}.
However, this is only a partial solution as instructions must still wait on instructions they are dependent on, reducing the potential improvements. 
It is therefore crucial to find new ways of addressing the bottlenecks of core-composition from a hardware perspective.
By proposing new solutions to these bottlenecks, core-composition can become more effective at improving the performance of programs.
Introducing new hardware can even help dynamic multi-core processors become more adaptable to new types of programs, making them more practical.

\vspace{-1em}
\section{Contributions}
This thesis tackles the three problems described through the use of different techniques.

\paragraph*{Reconfiguration}
First, this thesis looks at the problem of how to automatically reconfigure the processor using techniques that learn from previous executions of programs.
A set of machine-learning models that are able to automatically make configuration decisions based on features of the software are proposed.
This thesis presents a design methodology for generating these models that use either program features extracted at the source code level or features that can be extracted at runtime, to determine when and how to configure the cores.
These models are not influenced by hand-picked heuristics, but instead are generated by exploring how different configurations and programs' features affect the affect performance and feeding this information to a machine-learning model.
By using machine learning, the processor can automatically be reconfigured ahead of time to improve the performance of multi-threaded applications, speeding up execution by 2.5x on average compared to a single core and up to 10x in the best case.
The thesis demonstrates that the processor can also be reconfigured at runtime to reduce energy consumption by 37\% on average whilst maintaining the speedup of the best static configuration.

\paragraph*{Maximising performance through software optimisations}
This thesis also provides an analysis of how different features of the source-code affect the speedup obtained via core-composition.
This is achieved by exploring a set of source-level optimisations and studying how this influences the performance of core-composition.
The analysis provides insights on when core-composition should be used, and how programs may be modified to increase the potential speedup.
It also motivates that the code modifications can be relatively  fast to implement; demonstrating that even without access to the source code, programs can be tuned to benefit from DMPs.

\paragraph*{Core composition mechanisms} 
Finally, thesis covers some of the current short-comings of core-composition from a hardware perspective.
The two main features of the hardware that are analysed are how instructions are fetched when cores are composed and how data-dependencies affect the performance of large core-compositions.
This thesis underlines that a simple, serial fetching scheme for instructions can be a considerable bottleneck when attempting to populate large core-compositions.
It also explores how data-dependencies limit the amount of ILP that large core-compositions can extract.
By providing a new fetching scheme that parallelises instruction fetches, and a value predictor which can speculate the results of instructions, this thesis shows that the performance of core-composition can be improved.
The thesis shows promising results: the performance (speedup) of core-composition can be improved by 1.87x, and up to 3x in the best case with perfect value and branch prediction; whereas using a state of the art value predictor still leads to an average performance increase of 1.33x and up to 2.7x.
This motivates further research into value prediction.

\section{Structure}
The overall aim of this thesis is to provide methods of making DMPs more practical, from automated reconfiguration to new hardware that improves the performance of the DMP.
The structure of thesis is as follows:

\textbf{Chapter ~\ref{chp:Background}} provides information on the different topics used throughout this thesis. The topics involve the reconfiguration mechanisms of DMPs, the instruction set architecture used, how value prediction works and the different machine-learning techniques employed throughout this thesis.

\textbf{Chapter ~\ref{chp:rw}} presents the related work. This covers previously proposed DMP processors and the different offline and online reconfiguration schemes that have been explored. 
This is followed by a discussion of work done on compiler optimisations, the different hardware techniques that improve energy efficiency, other proposed value predictors and different types of speculative hardware.

\textbf{Chapter ~\ref{chp:setup}} covers the setup of the simulator used throughout this thesis and the benchmarks that are explored.

\textbf{Chapter ~\ref{chp:streamit}} shows how the process of configuring a DMP to improve the performance of multi-threaded streaming applications can be learned.
The chapter demonstrates that a mix of core-composition and multi-threading is required to get the best speedup for these applications.
A machine-learning model is then trained to determine a good configuration of the processor based on source-code information derived from the application.
This chapter is based on the work previously published at LCTES 2016 ~\cite{micolet2016dmpstream}.

\textbf{Chapter ~\ref{chp:cases}} uses dynamic reconfiguration to reduce energy consumption whilst maintaining the same performance as the optimal static ahead of time configuration.
The chapter first covers some of the factors that affect the performance of core-compositions, such as branch prediction accuracy requirements.
This is followed by a study of how runtime adaptation of the processor can reduce energy consumption.
A machine-learning model that can determine the correct size of a core-composition at runtime, based on the types of instruction being executed is then designed.
This proves that dynamic reconfiguration can also be learned.
The chapter is based on the work previously published at CASES 2017 ~\cite{micolet2017cases}.

\textbf{Chapter ~\ref{chp:hardchanges}} presents new hardware that allows larger core-compositions to perform better in the average situation by reducing latencies caused by executing code on such compositions.
The new additions involve a new fetching mechanism that ensures each core can fetch instructions independently and in a round-robin fashion and the use of value prediction to minimise stress on the network on chip and reduce the effect of data dependencies between cores.
The chapter shows that a current state-of-the art value predictor paired with the new fetching mechanism can outperform the current implementation of core-composition.

\textbf{Chapter ~\ref{chp:conclusion}} finally concludes this thesis by summarising the contributions, providing critical analysis and presenting future work in the field of DMPs.
