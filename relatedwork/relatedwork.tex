\chapter{Related Work}~\label{chp:rw}
Whilst dynamic multicore processors (DMPs) are relatively new in terms of processor design they are still a solution to a well researched set of problems.
These problems include improving performance of single-threaded applications and reducing energy consumption.
This chapter covers the related work relevant and adjacent to this thesis.
%The topics covered are:
%\begin{itemize}
%\item The different dynamic multicore processors using core composition proposed in recent research.
%\item Work on determining when to change the size of a core composition either ahead of time or at runtime.
%\item Research on different hardware techniques for saving energy such as dynamic voltage and frequency scaling or using heterogeneous chip multicore processors.
%\item Research in optimising code for core composition and EDGE.
%\item Other models of speculative execution.
%\item Work on streaming programming languages.
%\item Current research in value prediction and data pre-fetching.
%\item Work on machine learning guided optimisations for both hardware and software.
%\end{itemize}

\section{Reconfigurable Processors}

This section covers work on both dynamic multicore processors (DMP) and processors that can reconfigure their microarchitectures.

\subsection{Dynamic Multicore Processors}
The idea of composing physical cores was first introduced by Ipek et al. in CoreFusion~\cite{ipek2007CoreFusion}.
CoreFusion employs a traditional architecture with 2 issue out of order cores.
When cores are fused, they collectively fetch from the same thread, whenever an instruction cache miss is issued, an eight word block is delivered and distributed across all of the cores' caches.
Fetches are aligned with the core responsible for the 2 oldest instructions.
In the original paper, an 4 core composition obtains a 1.3x speedup on SPECINT and a 1.5x speedup on SPECFP over a 2 issue core.
 
The Bahurupi~\cite{pricopi2012bahurupi} polymorphic heterogeneous multicore architecture proposed by Pricopi et al. introduces a sentinel instruction which informs the hardware about the \textit{live-in} and \textit{live-out} registers of a basic block.
Baharupi uses the SimpleScalar ISA~\cite{burger1997simplescalar}, and the compiler adds this instruction to the top of every basic block, splitting the program up into blocks similarly to EDGE.
They show that, on average this introduces a 24\% code size increase for SPEC integer applications, 15\% for SPEC floating point and 19\% for Mediabench~\cite{mediabench} and MiBench~\cite{mibench} benchmarks.

Another important piece of information contained in this instruction is the size of a block.
Cores in a composition fetch basic blocks in a similar fashion to the one described in Chapter~\ref{chp:Background} Section~\ref{chp:Background:sec:EDGE}.
However, cores must execute global register renaming in the sequential order of blocks.
To ensure sequential renaming, a Global Program Counter (GPC) is introduced, and each core in the composition must lock the GPC before fetching a block and doing the renaming; this makes Baharupi less efficient at fetching blocks than EDGE.
Using a 4 core composition they report a performance improvement of 2x on SPECINT,3x on SPECFP and 4x on Mibench/Mediabench compared to a 2 issue core. 

TFlex~\cite{kim2007tflex} proposed by Kim et al. is an EDGE processor that also deploys core composition.
The mechanism is similar to the one described in Chapter~\ref{chp:Background} Section~\ref{chp:Background:sec:EDGE}.
They motivate that EDGE simplifies distributing instructions across cores as EDGE itself is designed for distributed microarchitectures.
Unlike the model used throughout this thesis, a block's instructions can be distributed amongst the cores in the composition, with one of the cores being the block owner.
As each core can be responsible for a single block, the total number of blocks in flight is equal to the number of cores in the composition.
In their work~\cite{kim2007tflex} Kim et al. show that a 32 core composition can outperform a single TFlex core by 3x on a set of EEMBC benchmarks and SPEC2000 microbenchmarks.

The E2 processor is another EDGE based processor that can dynamically compose its cores~\cite{putnam2010e2}.
Unlike TFlex, when cores are fused in the E2 processor, each core is responsible for the blocks they fetch.
E2 introduces the concept of segmenting the instruction window into lanes, allowing cores to fetch multiple blocks.
The processor explored throughout this thesis is based on the E2 processor.

Watanabe et al. propose a different type of dynamic multicore processor, where cores share execution units ~\cite{Watanabe2010Widget}.
The Wisconsin Decoupled Grid Execution Tiles (WidGET) architecture's design is based on a sea of resources, where a core is composed of a simple Instruction Engine (IE) that can send instructions to a set of available Execution Units (EU) that execute instructions in order.
This allows for fine-grained reconfiguration of the cores: depending on the available ILP, an IE can increase the number of EUs it needs if there's a high amount of ILP available, or inversely reduce it.
Watanabe et al. compare the performance of their processor to that of an Intel Atom and Intel Xeon using the SPEC2006 benchmark suite.
They show that using in-order cores with a fine-grained reconfiguration can reduce power consumption by 21\% compared to the Xeon whilst achieving the same performance.
It is even able to outperform it by 26\% whilst still reducing power consumption by 6\%.
 
\subsection{Reconfigurable microarchitectures}

MorphCore~\cite{khubaibMorphCore2012} focuses on reconfiguring a core for thread level parallelism.
It switches between out-of-order (OoO) when running single threaded applications and an in-order core optimised for simultaneous multi threading (SMT) workloads.
This provides an opposite solution to our DMP: providing a large core made for ILP that can be modified to better fit TLP workloads.
MorphCore outperform a 2-Way SMT OoO core by 10\% whilst being 22\% more efficient.

ElasticCore~\cite{tavanaElastic} proposes a morphable core that uses dynamic voltage and frequency scaling (DVFS) and microarchitectural modifications such as instruction bandwidth and capacity to adapt the processor to current needs.
Unlike heterogeneous systems that can present different sized cores on a single package, the ElasticCore is a single core with four different size configurations.
Each size configuration uses more resources than the previous, such as increasing the fetch width from 2 to 4 to 8 instructions.
Having all the resources on the single core allows adaptation to be quicker than migrating a thread to a  more appropriate core, 1000 cycles compared to the 10,000 for ARM's big.LITTLE architecture.

This is similar to the work proposed by Dubach et al. in ~\cite{dubach13dynamic} where microarchitectural features can be modified for better performance or energy efficiency.
They provide extensive analysis of SPEC 2000 benchmarks and demonstrate that machine learning and dynamic adaptation can double the energy/performance efficiency compared to a static configuration.


\section{Automated processor reconfiguration}

In Ipek et al's. original work on CoreFusion~\cite{ipek2007CoreFusion} they introduce the \textit{FUSE}/\textit{SPLIT} instructions that allow for dynamic reconfiguration.
However, their eight core system only has two possible configurations: either each core is executes on their own or they are fused into groups of four cores; no details as to when to compose cores is discussed.
This is the same for the original proposal for TFlex~\cite{kim2007tflex}, where 5 different configurations are explored (2, 4, 8, 16 and 32 composed cores), but does not provide any insight as to how to determine the composition size automatically.

In the work of Pricopi et al.~\cite{pricopiSchedCoreComp2014}, they show how dynamic reconfiguration is beneficial when it comes to scheduling multiple tasks.
However, they do not discuss any method of automatically deciding the optimal configuration beyond a 4 core composition.
In their work they use speedup functions determined from profile executions of applications to determine how to schedule tasks.
This means that whenever an application is modified, it must be re-analysed to benefit from dynamic composition.
They do not discuss what software characteristics help determine when to reconfigure the cores, or how to optimise software.

Gulati et al. propose an offline and online scheduling algorithms for Tflex \cite{gulati2008multitaskingdmc}. 
This work focuses on maximising speedup for a set of workloads in a multi-tasking environment.
The offline model uses profiling data to make decisions whereas the online model measures the current performance of a task being executed and switches the size of the composition based on a threshold.
In their results they show that the offline profiling tool outperforms the online algorithm.
As the threshold is set ahead of time, the online model is not able to fully utilise the system when applications do not meet the required threshold.
They demonstrate that a DMP that can adapt to workloads can result in faster response times than a tradition CMP, between 21\% to 13x faster.

%In~\cite{santos2013nocdmc} they use information provided by the application to determine how to reconfigure some components of the processor.
%This initial information then assists the rest of the reconfiguration, this process still requires input from the programmer.

\section{Code optimisation for EDGE}

Whilst there exists no literature on code optimisations geared towards improving the performance of core compositions, some previous work exists on studying how block sizes affect the performance of the EDGE architecture.
Smith et al. highlight the importance of block size in their work on an EDGE compiler ~\cite{smith2006edge} stating that larger blocks will lead to better performance.
They suggest the use of instruction predication~\cite{smith2006dataflowpred} that allows multiple EDGE blocks to be fused into a single block, also known as a hyperblock.

Since EDGE instructions pass their results directly to an instruction's input operands, some optimisations are required to ensure that predicates are efficiently broadcasted to all depending instructions.
The optimisations are predicate fanout reduction, path-sensitive predicate removal and instruction merging~\cite{smith2006dataflowpred}.
Overall, hyperblocks are able to improve the performance of a set of EEMBC benchmarks by 29\% compared to only using basic blocks.
Using the optimisations previously defined improves the performance of hyperblocks by up to 12\% compared to non-optimised hyperblocks.


\section{Hardware techniques for power and energy efficiency}

This section covers different hardware techniques used for energy efficiency that do not use core composition or other reconfigurable system.

\subsection{Dynamic Voltage and Frequency Scaling}
Dynamic Voltage and Frequency scaling (DVFS) is a method of modifying the power and energy consumption~\cite{paganiEECHM2017} by modifying the voltage or the clock rate of the processor.
Often times, DVFS is used to reduce energy or power consumption in phases of low performance.

Herbert et al in ~\cite{herbertDVFS07} demonstrate that DVFS is an effective technique for reducing $energy/throughput^2$ ($E/T^2$).
They suggest two methods of using DVFS: controlling it at a per-core basis, or at a cluster basis, also known as Voltage Frequency Islands (VFI).
Using VFIs reduces the design complexity of both the hardware and algorithms that control DVFS.
They demonstrate that on a 16 chip multicore processor (CMP), DVFS can reduce $E/T^2$ by 38.2\% on a set of multithreaded workloads.
The results also highlight how core-level DVFS does not reduce $E/T^2$ significantly compared to using VFIs: 38.2\% for core-level, compared to 37.9\% for a 4 core VFI.

Pagani et al. consider the use of VFIs in a heterogeneous system in ~\cite{paganiEECHM2017}, where cores in a VFI are homogeneous, but the different VFIs are heterogeneous.
Their objective is to ensure that tasks are running on the most energy efficient core whilst satisfying the time constraints of the task.
Once a task is partitioned and dispatched onto a core in a VFI cluster, Pagani et al. use Single Frequency Approximation (SFA)~\cite{sfaScheme} as the DVFS strategy.
SFA determines a single voltage/frequency that satisfies timing constrains of a task.


Vega et al. in ~\cite{vega2013crank} underline that whilst DVFS is an effective way of reducing power and energy consumption, the fact that it is decoupled from other techniques such as per core power-gating (PCPG) reduces the overall benefits.
They suggest an online algorithm at the OS level that collects data from performance counters and makes multiple decisions based on the data gathered.
%write more
%However this approach is orthogonal to DMPs~\cite{sibi2014}, whilst both techniques (DVFS and core composition) adapt to programs phases, DMPs can also be used to speed up the execution of programs.

\subsection{Thread migration in HCMPs}
ARM big.LITTLE~\cite{armbig} is an example of a heterogeneous chip multicore processor (HCMP) that provides two different types of cores to allow the programmer to choose between energy efficiency and performance.
A program can be migrated from one core to another depending on the requirements, however this comes at the cost of a very high migration overhead, over 10,000 cycles~\cite{armbig}.
However Gutpa et al. show that by selecting the correct core configuration at runtime this can lead to an energy reduction of up to 46\%

%More on HAQUE
Haque et al. show that an HCMP is able to reduce energy consumption and improve throughput for applications with high-percentile latencies~\cite{tailAMP2017}.
They highlight that service provides receive requests of different lengths of computation.
By successfully scheduling the different lengths to the correct cores in an HCMP, they can reduce the energy consumption of short requests by a factor of 50\% compared to DVFS.

Adileh et al. argue that current proposals for scheduling tasks on small or large cores are inadequate when operating on a power-constrained HCMPs in a multi-tasking environment~\cite{adileh2016power}.
Using linear programming they determine that applications can be ranked based on their delta performance/delta power (DPDP) which is defined as the ratio of performance and power difference between executing an application on a small or large core.
Applications that ranked highly are executed on the large core whilst others are executed on the smaller cores.
After defining five schemes that used DPDP to schedule tasks within a power budget of 1W per second per application they show that it outperforms other schemes by 16\% on average and up to 40\%.
 
Gupta et al. tackle the issue of the high number of configurations possible in an HCMP by characterising workloads offline~\cite{Gupta2017Dypo}.
Applications are executed and their performance is recorded on the different available cores, with different parameters tuned such as the core's clock frequency.
Then, for different optimisation goals they found the Pareto-optimal configuration for each of the benchmarks.
This information is then used to build a classifier that can determine the correct core configuration for a given snippet of an application.
Using this classifier Gupta et al. are able to improve performance per watt of a set of single and multithreaded benchmarks by 93\%, 81\% and 6\% compared to an interactive, on demand and powersave governor respectively.

\section{Speculative Execution}
Core composition is able to improve performance of applications by executing many instructions speculatively from the same thread.
This section covers work on speculative parallelism, where performance improvements are obtained by speculatively executing multiple tasks in parallel, rather than through deep branch prediction.

\paragraph*{Software level}
The idea of speculatively extracting parallelism at runtime was first introduced by Rauchwerger et al. in~\cite{runtimeSpec}.
They underlined that compile-time analysis of single-threaded programs does not allow for the complete detection of parallel sections, and must be complemented by runtime analysis.
They proposed a framework for parallelising loops at runtime: instead of detecting if the loop is parallelisable or not, the loop is speculatively executed in parallel and then runtime analysis is conducted to verify that no data-dependencies are violated.
If any violations occur, the loop is re-executed serially.
Loops must be marked as speculatively parallel by the compiler, the run-time system only verifies whether or not the speculative execution does not violate dependencies.

Hertzberg et al push the idea of speculative multithreading further in ~\cite{dbtspec2011} by using dynamic binary translation (DBT) to generate optimised parallel code on the fly, they name this the Runtime Automatic Speculative Parallelism technique (RASP).
Instead of generating speculative parallel loops at compile time, they propose that idle cores should be used to analyse running programs and generate parallel versions of the loops.
The code continues to be analysed even after the generation of parallel loops in order to ensure that the optimal code has been generated.
Using RASP, Hertzberg et al demonstrate that their system can lead to a performance increase average of 1.46x on SPEC2006 integer benchmarks and up to a 3x speedup on SPEC2006 floating point benchmarks compared to single-threaded execution.

Whilst the work proposed by Hertzberg et al. motivate pairing DBT with speculative parallelism, Koch et al. show in ~\cite{koch2013spec} that the majority of the speedup obtained arises from the DBT optimisations and not speculative parallelism.
As serial programs can also benefit from DBT optimisations, Koch et al. argue that the work in Hertzberg et al. does not necessarily motivate dynamic binary parallelisation (DBP) but instead DBT.
Without DBT, RASP only provides a 1.12x speedup on SPEC2006 integer benchmarks, compared to the 1.46x when DBT optimisations are turned on.
Koch et al. underline that the cost of detecting loops which can be parallelised, paired with the cost of starting threads can often outweigh the performance benefits of DBP.
 
\paragraph*{Hardware level}
Jeffrey et al. present a novel tiled architecture called Swarm~\cite{swarm2016} that performs aggressive thread-level speculation.
The Swarm architecture executes tasks that are identified via timestamps.
Each task can access any data and has the ability to generate new tasks, also known as children, that will be assigned a greater timestamp.
All tasks are maintained by a task-queue, and can be executed out of order.

Swarm is extended by Abeydeera et al. in their speculation-ware multithreading policy SAM~\cite{Abeydeera2017SpecMulti}.
According to Abeydeera et al. having a high number of speculative threads often leads to a large amount of aborted tasks which impacts performance
SAM extends Swarm by ensuring that tasks with lower timestamps are prioritised as they are most likely to commit, thus reducing the number of aborted tasks.
This is achieved by performing issue stage prioritisation.
They also relax conflict resolution by using a similar technique to Wait-n-GoTM~\cite{waitNGo2013}.
Instead of assigning tie-breakers to tasks when they are spawned, SAM assigns them when a task acquires a dependence from another task with equal timestamp; this reduces the number of needless aborts.
Overall, an 8 in order core Swarm processor with SAM improves performance by 2.33x compared to a single core for a set of graph algorithms.

Subramian et al. present a new execution model that supports nested parallelism and is implemented for a Swarm~\cite{fractal2017} .
Fractal introduces the concept of grouping tasks into hierarchies of nested \textit{domains}.
Tasks in a domain can either execute in order or out of order relative to their timestamps and appear to execute as a single atomic unit to other domains.
By allowing tasks to generate domains, Fractal is able to exploit nested parallelism without complicating the software, and can lead to a performance increase of up to 88x compared to Swarm on some graph algorithms.

\section{Value Prediction}
%The value predictor used in Chapter~\ref{chp:hardchanges} is the block based D-VTAGE predictor \cite{peraisBeBop2015}.
%It is a \textit{computational} value predictor that is adapted from the VTAGE value predictor also presented by Perais et al~\cite{peraisVTAGE2014}.
The earliest works can be retraced to Mikko et al. where they propose a \textit{Load Value Predictor} \cite{lipasti96valpred} that is able to predict the value of load instructions.
They show that performance can be improved by up to 27\% by only predicting load values.

Perais et al propose the VTAGE \textit{context} value predictor ~\cite{peraisVTAGE2014}, that by adopts the same prediction scheme as the ITTAGE branch predictor~\cite{SeznecITTAGE}.
Using global branch history, which is easier to maintain that data-flow history~\cite{peraisVTAGE2014}, VTAGE can improve performance of some SPEC applications by up to 65\%.

The block based D-VTAGE predictor~\cite{peraisBeBop2015} can quickly issue multiple predictions grouping up predictions as a single block.
The basic prediction fetch and update mechanism are inherited from VTAGE, except D-VTAGE is a \textit{computational} value predictor.
In order to improve value prediction for tightly knit loops where multiple iterations of the loop body can be live in parallel, the predictor employs a speculative window that is able to keep track of live speculative data.
D-VTAGE is able to obtain up to a 1.7x speedup, and averages a 1.10x speedup on a set of SPEC applications. 

Miguel et al. propose a different technique for value prediction called load value approximation \cite{miguel2014LoadVal} for applications where value inexactness is acceptable.
Applications such as image tracking or image comparison do not need to operate on exact values as they often allow for a margin of error.
Therefore, these applications do not need to roll-back on a mispredicted value, and can continue to operate with incorrect data as long as it is sufficiently accurate.

Sheikh et al. present a value predictor that is able to avoid mispredictions caused by Load $\,\to\,$ Store $\,\to\,$ Load conflicts~\cite{sheikh2017value}.
This is achieved by predicting load memory address at the instruction fetch stage and checking the data-cache for that memory address.
If the memory address is contained in the data-cache, then the value is is fetched (this is considered a prediction).
If the address is not contained in the cache, then a data prefetch is issued.
Their new approach to value prediction generates a 4.8\% speedup on a set of SPEC, EEMBC and Octane applications.
This is almost 2x more than the VTAGE predictor used in the paper (2.1\%).

\section{Data Prefetching}
Whilst value prediction is used to mask data dependencies or long latencies caused by cache misses by predicting values, data prefetching attempts to make data available in L1 caches before it is needed.
This can be achieved both in software by inserting instructions that trigger a data fetch-request ahead of time, or in hardware by analysing memory access patterns.

Ainsworth et al. in ~\cite{graphPrefetch2016} suggest that stride-based prefetcher is not adequate for irregular memory accesses.
They propose a hardware prefetcher that has knowledge on the data structures being treated, to be able to trigger multiple prefetches based on loads snooped on the L1 cache.
Using Breadth-First-Search (BFS) as an example with the compressed sparse row data-format, they demonstrate that a specialised prefetcher can improve performance by a factor of 2.3x compared to stride prefetching and software prefetching that only generates a 1.1x speedup.

Prefetching for indirect memory accesses is considered difficult when using a hardware prefetcher~\cite{lee2012whenprefetchworks,prefetchForIndirect2017}.
Thus Ainsworth et al. also propose a compiler pass that inserts non-blocking loads to enable prefetching for indirect memory accesses in ~\cite{prefetchForIndirect2017}.
The compiler pass target loads inside loops whose address depends on a loop induction variable and meet a set of conditions to ensure that the prefetches do not cause faults.
Once the loads have been detected, the prefetches are scheduled based on a formula they devise in the paper.
The formula takes into account the number of loads found in the prefetch sequence for the loop, the position of the real load, and a constant that is based on architectural features such as memory-latencies and the possible IPC.
Using their compiler pass they are able to improve the average performance of memory intensive benchmarks by 1.1x to 2.7x on different processors.

\section{Dataflow Programming Languages}

StreamIt~\cite{theis2002streamit} is one of the first programming languages directed towards streaming applications.
As previously described in Chapter~\ref{chp:Background} Section~\ref{sec:bg:stream}, StreamIt defines a set of constructs to build scalable parallel streaming applications.
StreamIt was originally intended for the RAW ~\cite{waingold1997raw} tile-based architecture, but can be used in other settings as well.

Brook~\cite{buck2004brook} is another streaming programming language geared towards Graphical Processing Units (GPUs).
Unlike StreamIt, Brook extends the C language by providing a new datatype and function types.
The new datatype, called a \textit{stream}, provides a collection of data which can be operated on in parallel, which can be modified by stream functions.
A stream function takes one or more stream inputs and will output one or more streams; this is similar to a StreamIt \textit{filter}.
To operate on streams in parallel, Brook defines a subset of stream functions called \textit{kernel} functions.
These functions do not have access to global values, cannot call functions that aren't kernel functions as well, and are only allowed to access streams in a read-only \textit{OR} write-only way.
This is to facilitate the compiler generation of dataflow graphs for the program.

WaveScript on the other hand is developed for embedded systems~\cite{newton2008wavescript}.
Unlike StreamIt, WaveScript uses an asynchronous streaming model, where the input and output rates of functions are not known at compile time.
%Programs are not defined as graphs, instead 

Bosboom et al. demonstrate how a streaming language can be directly embedded into a more common host-language in ~\cite{bosboom2014streamjit}.
They present StreamJIT which inherits StreamIt's programming structure, however the language is embedded inside of Java.
This allows the new StreamJIT language to benefit from the front-end compiler optimisations developed for Java; a method they call \textit{commensal} compilation.

\section{Partitioning streaming programs on multicore chip}

Previous work on scheduling streaming applications onto DMPs or heterogenous multicore chips focuses on finding mathematical ways of partitioning the graph onto the chip ~\cite{carpenter2009streammap,kudlur2008orchestratingstreamprog}.  
In Carpenter et al.'s work~\cite{carpenter2009streammap} they restrain themselves to partitioning a StreamIt application maintaining connectedness.
Connectedness can be defined as a subgraph where the filters are connected. 
This restriction reduces the number of potential partitions that can be generated by their algorithm and will put TLP in favour of ILP. 

Kudlur et al. in~\cite{kudlur2008orchestratingstreamprog} choose to represent the partitioning problem as an integer linear programming problem.
They start by fissionioning stateless filters to obtain the optimal load balance across all cores and assign the filters to a core using a modulo scheduler.

Farhad et al. also use integer linear programming in~\cite{farhad2012streamilp} to schedule StreamIt programs on multicore.
They profile the communication costs of the streaming programs by running the program using different multicore allocations and feed that information into their integer linear programming model.

Using a machine learning model to partition StreamIt programs was previously explored in the work of Wang et al. in ~\cite{wang2013partitionstreamit}.
They use a k nearest neighbor model to determine the perfect partitioning of a StreamIt program for a multicore system. 
Their model is used to find ways of fusing and fissioning filters to discover a new graph that can then be mapped onto a multicore system.


\section{Machine-learning guided performance optimisations}

Using machine learning to increase performance has been a popular area of research as of late.
There are two areas in which machine learning is used: compiler driven optimisations and runtime driven optimisations.

Dubach et al. use machine learning to determine what the performance and energy consumption will be based on the microarchitectural parameters and compiler optimisations that are used in ~\cite{DubachExpl2012}.
By exploring the 35 MiBench benchmarks and modifying compiler optimisations and microarchitectural features of an embedded system.
Over 1000 compiler flags are explored, and 200 configurations of the processor are used (modifying cache sizes, associativity, branch target buffer sizes and associativity).
Their design space exploration shows that to obtain the best performance for each of the applications, both the compiler optimisations and processor must be tuned.
They then use this information to build a machine learning model using an Artificial Neural Network which is able to predict the performance, in terms of $Energy \times Delay \times Delay$ of an application given a set of compiler flags and microarchitectural features.
Their model is able to predict a performance that with an error rate of just 3.2\%.

Cummins et al. developed a deep learning model that takes source-code and is able to learn how the code correlates to performance~\cite{cummins2017pact}.
Their deep learning model ingests source-code and through a set of transformations, and through training the model, it is able to also generate optimisation heuristics.
They train their deep learning model for two scenarios: GPU thread-coarsening and CPU/GPU task partitioning.
Compared to state of the art hand-crafted heuristics, the deep learning model is able to outperform both scenarios by 12\% and 14\% respectively.