\chapter{Related Work}
This chapter covers the related work relevant to this thesis.
The topics covered are:
\begin{itemize}
\item The different dynamic multicore processors using core composition proposed in recent research.
\vspace{-1em}
\item Work on determining core composition sizes.
\vspace{-1em}
\item Research on different hardware techniques for saving energy.
\vspace{-1em}
\item Work on streaming programming languages.
\vspace{-1em}
\item Work on machine learning guided optimisations in both hardware and software.
\vspace{-1em}
\item Current research in value prediction.
\end{itemize}

\section{Reconfigurable Processors}

This section covers work on both dynamic multicore processors (DMP) and processors that can reconfigure their microarchitectures.

\subsection{Core composition}
The idea of composing physical cores was first introduced by Ipek et al. in CoreFusion~\cite{ipek2007CoreFusion}.
CoreFusion employs a traditional architecture with 2 issue out of order cores.
When cores are fused, they collectively fetch from the same thread, whenever an instruction cache miss is issued, an eight word block is delivered and distributed across all of the cores' caches.
Fetches are aligned with the core responsible for the 2 oldest instructions.
In the original paper, an 4 core composition obtains a 1.3x speedup on SPECINT and a 1.5x speedup on SPECFP over a 2 issue core.
 
The Bahurupi~\cite{pricopi2012bahurupi} polymorphic heterogenous multicore architecture proposed by Pricopi et al. introduces a sentinel instruction which informs the hardware about the \textit{live-in} and \textit{live-out} registers of a basic block.
Baharupi uses the SimpleScalar ISA~\cite{burger1997simplescalar}, and the compiler adds this instruction to the top of every basic block, splitting the program up into blocks similarly to EDGE.
They show that, on average this introduces a 24\% code size increase for SPEC integer applications, 15\% for SPEC floating point and 19\% for Mediabench~\cite{mediabench} and MiBench~\cite{mibench} benchmark suites.

Another important piece of information contained in this instruction is the size of a block.
Cores in a composition fetch basic blocks in a similar fashion to the one described in Chapter~\ref{chp:Background} Section~\ref{chp:Background:sec:EDGE}.
However, cores must execute global register renaming in the sequential order of blocks.
To ensure sequential renaming, a Global Program Counter (GPC) is introduced, and each core in the composition must lock the GPC before fetching a block and doing the renaming; this makes Baharupi less efficient at fetching blocks than EDGE.
Using a 4 core composition they report a performance improvement of 2x on SPECINT,3x on SPECFP and 4x on Mibench/Mediabench compared to a 2 issue core. 

TFlex~\cite{kim2007tflex} proposed by Kim et al. is an EDGE processor that also deploys core composition.
The mechanism is similar to the one described in Chapter~\ref{chp:Background} Section~\ref{chp:Background:sec:EDGE}.
They motivate that EDGE simplifies distributing instructions accross cores as EDGE itself is designed for distributed microarchitectures.
Unlike the model used throughout this thesis, a block's instructions can be distributed amongst the cores in the composition, with one of the cores being the block owner.
As each core can be responsible for a single block, the total number of blocks in flight is equal to the number of cores in the composition.
In their work~\cite{kim2007tflex} Kim et al. show that a 32 core composition can outperform a single TFlex core by 3x on a set of EEMBC benchmarks and SPEC2000 microbenchmarks.

Finally, the E2 processor is another EDGE based processor that can compose its cores~\cite{putnam2010e2}.
Unlike TFlex, when cores are fused in the E2 processor, each core is responsible for the blocks they fetch.
E2 introduces the concept of segmenting the instruction window into lanes, allowing cores to fetch multiple blocks.
The processor explored throughout this thesis is based on the E2 processor.

\subsection{Reconfigurable microarchitectures}
ElasticCore~\cite{tavanaElastic} proposes a morphable core that uses dynamic voltage and frequency scaling (DVFS) and microarchitectural modifications such as instruction bandwidth and capacity to adapt the processor to current needs.
This is similar to the work proposed by Dubach et al. in ~\cite{dubach13dynamic} where microarchitectural features can be modified for better performance or energy efficiency.
They provide extensive analysis of SPEC 2000 benchmarks and demonstrate that machine learning and dynamic adaptation can double the energy/performance efficiency compared to a static configuration.

MorphCore~\cite{khubaibMorphCore2012} focuses on reconfiguring a core for thread level parallelism.
It switches between out-of-order (OoO) when running single threaded applications and an in-order core optimised for simultaneous multi threading (SMT) workloads.
This provides an opposite solution to our DMP: providing a large core made for ILP that can be modified to better fit TLP workloads.
MorphCore outperform a 2-Way SMT OoO core by 10\% whilst being 22\% more efficient.


\section{Automated reconfiguration for DMPS}

Very little work exists on determining the size of a core composition given an application, or when the composition must change.
In Ipek et al's. original work on CoreFusion~\cite{ipek2007CoreFusion} they introduce the \textit{FUSE}/\textit{SPLIT} instructions that allow for dynamic reconfiguration.
However, their eight core system only has two possible configurations: either each core is executes on their own or they are fused into groups of four cores; no details as to when to compose cores is discussed.
This is the same for the original proposal for TFlex~\cite{kim2007tflex}, where 5 different configurations are explored (2, 4, 8, 16 and 32 composed cores), but does not provide any insight as to how to determine the composition size automatically.

In the work of Pricopi et al.~\cite{pricopiSchedCoreComp2014}, they show how dynamic reconfiguration is beneficial when it comes to scheduling multiple tasks.
However, they do not discuss any method of automatically deciding the optimal configuration beyond a 4 core composition.
In their work they use speedup functions determined from profile executions of applications to determine how to schedule tasks.
This means that whenever an application is modified, it must be re-analysed to benefit from dynamic composition.
They do not discuss what software characteristics help determine when to reconfigure the cores, or how to optimise software.

Gulati et al. propose an offline and online scheduling algorithms for Tflex \cite{tflexmultitask}. 
This work focuses on maximising speedup for a set of workloads in a multi-tasking environment.
The offline model uses profiling data to make decisions whereas the online model measures the current performance of a task being exected and switches the size of the composition based on a threshold.
In their results they show that the offline profiling tool outperforms the online algorithm.
As the threshold is set ahead of time, the online model is not able to fully utilise the system when applications do not meet the required threshold.
Overall they demonstrate that a DMP that can adapt to workloads can result in faster response times than a tradition CMP, between 21\% to 13 times faster.

In~\cite{santos2013nocdmc} they use information provided by the application to determine how to reconfigure some components of the processor.
This initial information then assists the rest of the reconfiguration, this process still requires input from the programmer though.

\section{Hardware techniques for power and energy efficiency}

This section covers different hardware techniques used for energy efficiency that do not depend on core composition.

\subsection{Dynamic Voltage and Frequency Scaling}
Dynamic Voltage and Frequency scaling (DVFS) is a method of modifying the power and energy consumption~\cite{paganiEECHM2017} by modifying the voltage or the clock rate of the processor.
Often times, DVFS is used to reduce energy or power consumption in phases of low performance.

Herbert et al in ~\cite{herbertDVFS07} demonstrate that DVFS can be used to reduce $energy/throughput^2$ on a 16 chip multicore processor (CMP) by 39\% on a set of multithreaded workloads.
They argue that on a CMP, DVFS does not need to be on a per-core basis, as fine-grained DVFS does not improve $energy/throughput^2$ by much compared to coarse grained DVFS.

Vega et al. in ~\cite{vega2013crank} underline that whilst DVFS is an effective way of reducing power and energy consumption, the fact that it is decoupled from other techniques such as per core power-gating (PCPG) reduces the overall benefits.
They suggest an online algorithm at the OS level that collects data from performance counters and makes multiple decisions based on the data gathered.
%write more
%However this approach is orthogonal to DMPs~\cite{sibi2014}, whilst both techniques (DVFS and core composition) adapt to programs phases, DMPs can also be used to speed up the execution of programs.

\subsection{Heterogeneous Chip Multicore Processors}
ARM big.LITTLE~\cite{} is an example of a heterogeneous chip multicore processor (HCMP) that provides two different types of cores to allow the programmer to choose between energy efficiency and performance.
A program can be migrated from one core to another depending on the requirements, however this comes at the cost of a very high migration overhead, over 10,000 cycles~\cite{}.
However Gutpa et al. show that by selecting the correct core configuration at runtime this can lead to an energy reduction of up to 46\%

%More on HAQUE
Haque et al. show that an HCMP is able to reduce energy consumption and improve throughput for applications with high-percentile latencies~\cite{tailAMP2017}.
They highlight that service provides receive requests of different lengths of computation.
By successfuly scheduling the different lengths to the correct cores in an HCMP, they can reduce the energy consumption of short requests by a factor of 50\% compared to DVFS.

\section{Speculative Execution}
Core composition is able to improve performance of applications by executing many instructions speculatively from the same thread.
This section covers work on speculative parallelism, where performance improvements are obtained by speculatively executing mutliple tasks in parallel, rather than through deep branch prediction.

\paragraph*{Software level}
The idea of speculatively extracting parallelism at runtime was first introduced by Rauchwerger et al. in~\cite{runtimeSpec}.
They underlined that compile-time analysis of single-threaded programs does not allow for the complete detection of parallel sections, and must be completented by runtime analysis.
They proposed a framework for parallelising loops at runtime: instead of detecting if the loop is parallelisable or not, the loop is speculatively executed in parallel and then runtime analysis is conducted to verify that no data-dependencies were violated.
If any violations occur, the loop is re-executed serially.
Loops must be marked as speculatively parallel by the compiler, the run-time system only verifies whether or not the speculative execution does not violate dependencies.

Hertzberg et al push the idea of speculative multithreading further in ~\cite{dbtspec2011} by using dynamic binary translation (DBT) to generate optimised parallel code on the fly, they name this the Runtime Automatic Speculative Parallelism technique (RASP).
Instead of generating speculative parallel loops at compile time, they propose that idle cores should be used to analyse running programs and generate parallel versions of the loops.
The code continues to be analysed even after the generation of parallel loops in order to ensure that the optimal code has been generated.
Using RASP, Hertzberg et al demonstrate that their system can lead to a performance increase average of 1.46x on SPEC2006 integer benchmarks and up to a 3x speedup on SPEC2006 floating point benchmarks compared to single-threaded execution.

Whilst the work proposed by Hertzberg et al. motivated pairing DBT with speculative parallelism, Koch et al. show in ~\cite{koch2013spec} that the majority of the speedup obtained arises from the DBT optimisations and not speculative parallelism.
As serial programs can also benefit from DBT optimisations, Koch et al. argue that the work in Hertzberg et al. does not necessarily motivate dynamic binary parallelisation (DBP) but instead DBT.
Without DBT, RASP only provides a 1.12x speedup on SPEC2006 integer benchmarks, compared to the 1.46x when DBT optimisations are turned on.
Koch et al. underline that the cost of detecting loops which can be parallelised, paired with the cost of starting threads can often outweigh the performance benefits of DBP.
 
\paragraph*{Hardware level}
Jeffrey et al. present a novel tiled architecture called Swarm~\cite{swarm2016} that perfroms aggressive thread-level speculation.
The Swarm architecture executes tasks that are identified via timestamps.
Each task can access any data and has the ability to generate new tasks, also known as children, that will be assigned a greater timestamp.
All tasks are maintained by a task-queue, and cores can execute tasks out of order.

Swarm is extended by Abeydeera et al. in their speculation-ware multithreading policy SAM~\cite{Abeydeera2017SpecMulti}.
According to Abeydeera et al. having a high number of speculative threads often leads to a large amount of aborted tasks which impacts performance
SAM extends Swarm by ensuring that tasks with lower timestamps are prioritised as they are most likely to commit, thus reducing the number of aborted tasks.
This is achieved by performing issue stage prioritisation.
They also relax conflict resolution by using a similar technique to Wait-n-GoTM~\cite{waitNGo2013}.
Instead of assigning tie-breakers to tasks when they are spawned, SAM assigns them when a task acquires a dependence from another task with equal timestamp; this reduces the number of needless aborts.
Overall, an 8 in order core Swarm processor with SAM improves performance by 2.33x compared to a single core for a set of graph algorithms.

Subramian et al. present a new execution model for speculative parallelism called Fractal~\cite{fractal2017} which is also built on top of Swarm.



\section{Value Prediction}
%The value predictor used in Chapter~\ref{chp:hardchanges} is the block based D-VTAGE predictor \cite{peraisBeBop2015}.
%It is a \textit{computational} value predictor that is adapted from the VTAGE value predictor also presented by Perais et al~\cite{peraisVTAGE2014}.
The earliest works can be retraced to Mikko et al. where they propose a \textit{Load Value Predictor} \cite{lipasti96valpred} that is able to predict the value of load instructions.
They showed that a performance increase of up to 27\% can be achieved by only predicting load values.

Perais et al propose the VTAGE \textit{context} value predictor ~\cite{peraisVTAGE2014}, that by adoptes the same prediction scheme as the ITTAGE branch predictor~\cite{SeznecITTAGE}.
Using global branch history, which is easier to maintain that data-flow history~\cite{peraisVTAGE2014}, VTAGE can improve performance of some SPEC applications by up to 65\%.

The block based D-VTAGE predictor~\cite{peraisBeBop2015} can issue multiple predictions in a cycle by grouping up predictions as a single block.
The basic prediction fetch and update mechanism are inherited from VTAGE, except D-VTAGE is a \textit{computational} value predictor.
In order to improve value prediction for tightly knit loops where multiple iterations of the loop body can be live in parallel, the D-VTAGE predictor employs a speculative window that is able to keep track of live speculative data.
D-VTAGE is able to obtain up to a 1.7x speedup, and averages a 1.10x speedup on a set of SPEC applications. 

Miguel et al. propose a different technique called load value approximation \cite{miguel2014LoadVal} for applications where value inexactness is acceptable.
Applications such as image tracking or image comparison do not need to operate on exact values as they often allow for a margin of error.
Therefore, these applications do not need to roll-back on a mispredicted value, and can continue to operate with incorrect data as long as it is sufficiently accurate.

Sheikh et al. present a value predictor that is able to avoid mispredictions caused by Load $\,\to\,$ Store $\,\to\,$ Load conflicts~\cite{sheikh2017value}.
This is achieved by predicting load memory address at the instruction fetch stage and checking the data-cache for that memory address.
If the memory address is contained in the data-cache, then the value is is fetched (this is considered a prediction).
If the address is not contained in the cache, then a data prefetch is issued.
Their new approach to value prediction generates a 4.8\% speedup on a set of SPEC, EEMBC and Octane applications.
This is almost 2x more than the VTAGE predictor used in the paper (2.1\%).

\section{Data Prefetching}
Whilst value prediction tries to mask data dependencies or long latencies caused by cache misses by predicting values, data prefetching attempts to make data available before it is needed.
This can be achieved both in software by inserting instructions that trigger a data fetch-request ahead of time, or in hardware.

Ainsworth et al. in ~\cite{graphPrefetch2016} suggest that stride-based prefetcher is not adequate for irregular memory accesses.
They propose a hardware prefetcher that has knowledge on the data structures being treated, to be able to trigger multiple prefetches based on loads snooped on the L1 cache.
Using Breadth-First-Search (BFS) as an example with the compressed sparse row data-format, they demonstrate that a specialised prefetcher can improve performance by a factor of 2.3x compared to stride prefetching and software prefetching that only generates a 1.1x speedup.

They extend this work in ~\cite{eventTriggeredPrefetcher2018}


\section{Partitioning programs on multicore chip}

Previous work on scheduling streaming applications onto DMPs or heterogenous multicore chips focuses on finding mathematical ways of partitioning the graph onto the chip ~\cite{carpenter2009streammap,kudlur2008orchestratingstreamprog}.  
In Carpenter et al.'s work~\cite{carpenter2009streammap} they restrain themselves to partitioning a StreamIt application maintaining connectedness.
Connectedness can be defined as a subgraph where the filters are connected. 
This restriction reduces the number of potential partitions that can be generated by their algorithm and will put TLP in favour of ILP. 

Kudlur et al. in~\cite{kudlur2008orchestratingstreamprog} choose to represent the partitioning problem as an integer linear programming problem.
They start by fissionioning stateless filters to obtain the optimal load balance across all cores and assign the filters to a core using a modulo scheduler.

Farhad et al. also use integer linear programming in~\cite{farhad2012streamilp} to schedule StreamIt programs on multicore.
They profile the communication costs of the streaming programs by running the program using different multicore allocations and feed that information into their integer linear programming model.

\subsection{Machine Learning} 
Using a machine learning model to partition StreamIt programs was previously explored in the work of Wang et al. in ~\cite{wang2013partitionstreamit}.
They use a k nearest neighbor model to determine the perfect partitioning of a StreamIt program for a multicore system. 
The features we extracted using correlation analysis are similar to those presented in the work of ~\cite{wang2013partitionstreamit}.
Unlike our work their model is used to find ways of fusing and fissioning filters to discover a new graph that can then be mapped onto a multicore system.
Recent work on tuning software to hardware~\cite{cummins2017pact}.

\subsection{Heterogeneous Thread}
Read ~\cite{becchi2006ThreadOnCore}, ~\cite{adileh2016power}

\section{Dataflow Programming Languages}

There exist streaming languages that target different architectures.
For example Brook~\cite{buck2004brook} is designed to be used on GPUs and WaveScript for embedded systems~\cite{newton2008wavescript}.
These languages present different constructs to StreamIt, in particular they lack the graph oriented constructs. 
Lacking such constructs make these languages less attractive for tiled processors.


\section{Machine-learning guided optimisations}

Using machine learning to guide optimisations has been a popular area of research as of late~\cite{cummins2017pact,wang2018ml,dubach13dynamic}.
There are two areas in which machine learning is used: compiler driven optimisations and runtime driven optimisations.
Depending on the use, different models will have to be used; for example runtime systems require fast responses, thus the models will have to be smaller, whilst compiler driven optmisations are less pressured by this.
