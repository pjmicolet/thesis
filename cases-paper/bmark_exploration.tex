
\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{cases-paper/graphics/Exploration/ipcs_16_3.pdf}
    \caption{IPC as a function of time for each benchmark when run on 16 fused cores.}
    \label{fig:sxt}
	\vspace{1em}
\end{figure}

This section explores how core composition affects the performance of the SD-VBS benchmarks.
First a phase analysis is performed, followed by a study of the IPC variation for static core fusion.
Then the use of dynamic core composition is motivated by using the gathered information.
\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{cases-paper/graphics/Exploration/SSE_final.pdf}
    \caption{Sum of Square Errors given a number of kNN clusters for each of the benchmarks. The vertical line represents the number of clusters chosen for phase analysis.}
    \label{fig:sse}
		\vspace{1em}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{cases-paper/graphics/Exploration/clusters4.pdf}
    \caption{Number of phases determined for each benchmark using kMeans clustering and their distribution. The number above each phase represents the average observed IPC.}
    \label{fig:clust}
		\vspace{1em}
\end{figure}


\subsection{Phase Detection}
Figure~\ref{fig:sxt} presents the IPC performance through time for all the benchmarks when using a logical core (LC) composed of 16 cores.
The IPC is calculated for each time tick, which is set at interval of 640 blocks committed.
%make this way clearer
To re-iterate, the interval of 640 blocks was chosen as the largest core composition of size 16 can potentially commit up to 64 blocks at a time.
Thus measuring performance every 640 blocks allows to fully capture the performance of large compositions without sacrificing loss of information.
Displaying the performance of the LC of size 16 gives a performance ceiling for all the benchmarks as it is the maximum number of cores which can be fused at any point.
Figure~\ref{fig:sxt} shows that the IPC varies a lot for some of the benchmarks such as \bm{Disparity} or \bm{Localization} where dynamic fusion is expected to be especially good.
For other, such as \bm{Multi\_NCut}, the execution is dominated by a single long phase with constant IPC, which will clearly show no benefit from using dynamic fusion.

To better understand how dynamic core fusion improves performance, either by improving speedup or reducing energy, this section studies how each benchmark features different phases during their execution.
For every benchmark the IPC results of 16,8,4,2,1 fused cores are regrouped and kMeans clustering is applied to determine phases.
Intervals that exhibit similar IPC values when run on different core counts are classified in the same cluster.
In order to find the correct number of clusters the Sum of Square Errors (SSE) is plotted for a given cluster size from 1 to 15 (seen in Figure~\ref{fig:sse} and the optimal cluster is defined as the elbow in the plot~\cite{everitCluster2001}.
Applying a kMeans clustering to the IPCs of each application to determine phases is preferred to determining phases of an application by reading the source code phases.
This is because IPC is the determining metric for the phases used throughout this chapter.
For example, the benchmark \bm{MSER}'s source code has 7 distinct passes, yet the kMeans clustering shows that most of these passes result in the same performance, which can be visualised in Figure~\ref{fig:sxt}.
This kMeans clustering process is only done for the purpose of exploring this set of benchmarks.

Figure~\ref{fig:clust} shows the number of clusters for each benchmark and the number of times a tick in each cluster occurs throughout the execution of the program and the average IPC observed for each cluster.
The frequency of a cluster is counted by how many ticks in the program are part of that specific phase.
The data can be corroborated with the information found in Figure~\ref{fig:sxt}.
For example, benchmark \bm{Multi\_NCut} features one phase covering over 80\% of the total execution, whereas for \bm{MSER}, the two dominant phases have very similar IPC values.
This means that it will be impossible to obtain any kind of performance improvements through dynamic reconfiguration due to the fact that there is little opportunity to switch the size of the LC.
For all the other benchmarks, they each have at least two dominant phases.
Since each phase is a cluster of similar IPC values, having two or more clusters will result in a higher chance of benefiting from dynamic core fusion.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{cases-paper/graphics/Exploration/stddev3.pdf}
    \caption{Comparing average, smallest and greatest IPC for each SD-VBS benchmark using logical-core size of 16.}
    \label{fig:stddev}
		\vspace{5mm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{cases-paper/graphics/Exploration/SizeBuckets.pdf}
    \caption{Distribution of block sizes for each benchmark. The sizes are clustered in buckets equivalent to number of lanes occupied.}
    \label{fig:block_sizes}
	\vspace{5mm}
\end{figure}

\subsection{Static Core Fusion Exploration}

In this chapter, Static Core Fusion defines a processor configuration that is determined ahead of time and does not change once it has been set.
Figure~\ref{fig:stddev} shows how the average Instructions Per Cycle (IPC) changes as the size of a LC is increased, going in powers of 2 from 1 to 16 fused cores.
It can be seen that, for most benchmarks, fusing more cores provides an increase in IPC performance.
Benchmarks \bm{Disparity}, \bm{Localization}, \bm{Sift}, \bm{Stitch}, \bm{Texture Synthesis} and \bm{Tracking} all at least observe a speedup of 2x when using core fusion.

However increasing the size of a LC is not always beneficial as can be seen in benchmarks \bm{Localization}, \bm{MSER}, \bm{Multi\_NCut}, \bm{Stich}, and \bm{SVM}.
For benchmarks \bm{Localization} and \bm{Stitch} the performance increases when fusing up to 8 cores, where-as \bm{MSER} and \bm{Multi\_NCut} never benefit from core fusion.
These benchmarks may not see a reduction in performance on 16 cores due to the increase in network traffic, which will make the highest LC size slower than 8 cores.
Referring back to Figures~\ref{fig:sxt} and~\ref{fig:clust}, \bm{MSER} and \bm{Multi\_NCut} feature one dominating long phase, both performing poorly.

Figure~\ref{fig:stddev} also shows the variation of the IPC for each given LC size represented by the grayed out areas.
For example, running the \bm{Disparity} benchmark on a LC of 16 cores, it can be observed that an average IPC of 8.3 with a standard deviation of 5.2.
The variation for 16 cores shows that the performance can drop down to 2.5.
An IPC of 2.5 when using 16 cores is very inefficient as this represents 0.1 of an instruction per cycle for each core and this performance can be achieved by using smaller core compositions.
Using a LC of size 4 for the \bm{Disparity} benchmark an average of 4.1 with a standard deviation of 1.2 is achieved.
Thus, if the logical-core could change size, there is a possibility that this could reduce the overall energy consumption of the system by switching from 16 to 4.

Figure~\ref{fig:block_sizes} shows the distribution of block-sizes for each of the benchmarks.
As two blocks cannot occupy the same lane, the block sizes are clustered into groups which represent how ever many number of lanes will be occupied (one lane may execute a block of up to 32 instructions).
Figure~\ref{fig:block_sizes} helps explain why benchmarks \bm{MSER} and \bm{Multi\_NCut} do not perform any better with core-fusion.
For both cases, not only do they have a single detected phase, but both are predominantly formed of blocks that will occupy a single lane.
In fact, the most frequent block in \bm{MSER}, comprising 21\% the total executed blocks are comprised of only 8 instructions, whilst 31\% of \bm{Multi\_NCut}'s blocks are of 28 instructions.
Having such small blocks will always increase both the \textit{SynchronizationCost} and the branch-prediction requirements.
For \bm{MSER}, the fact that an important percentage of blocks are only 8 instructions long signifies that the overhead of fetching enough blocks for a large core-composition and the synchronization cost for committing them outweighs executing the blocks on a single core.
The reason there is no degredation of performance is due to the fact that when fusing a high amount of cores, if the overhead of fetching the blocks outweighs the time required to execute then a core will not submit a PC to another core in the composition.
In this case, multiple cores may be composed, but only a single one is executing any code.
For example, fusing 16 cores and executing \bm{MSER} may in fact result in a single core being used due to it executing blocks faster than it being able to dispatch the blocks to a next core.
Hence, in this case, \bm{MSER} would be wasting a lot of energy trying to use 16 cores in a composition, whilst effectively only executing on a single core. 
This explains the lack of scaling for these two benchmarks.
The source-code of \bm{MSER} and \bm{Multi\_NCut} is explored later on, to demonstrate how such small blocks are generated and why it cannot be improved.

Overall, most benchmarks that benefit from large logical-cores will also be met with important standard deviations of IPC performance.
The high standard deviation is evidence of performance phases found in each application which are likely to benefit from dynamic adaptation.

%Need a better name for this
\subsection{Analysis of code}

\begin{table}[t]
  \small
  \centering
 \begin{tabular} { | l | l | l | l | l | }
 \hline
   \cellcolor[gray]{0.7}Disparity & \cellcolor[gray]{0.7} Localization& \cellcolor[gray]{0.7} MSER& \cellcolor[gray]{0.7} Multi\_NCut& \cellcolor[gray]{0.7} Sift\\ \hline
	98\%  & 95\% & 85\%  & 100\%& 99\%\\ \hline
	 \cellcolor[gray]{0.7} Stitch & \cellcolor[gray]{0.7} SVM & \cellcolor[gray]{0.7} Text. Synth & \cellcolor[gray]{0.7} Tracking&\\ \hline
	  95\%& 93\%& 98\%& 98 \%&\\ \hline
	\end{tabular}
  \caption{Branch prediction accuracy in percentage for each of the benchmarks.}\label{tab:sd-vbsbpred}
  \vspace{1em}
\end{table}

As previously mentioned, both \bm{MSER} and \bm{Multi\_NCut} do not benefit from core-composition during their predominant phases'.
This subsection focuses on understanding why this is the case and explores the code that represent the main phases of both of these applications to underline where the problems arise.
%Other code snipets from other SD-VBS benchmarks are also explored to demonstrate examples of code that perform well on core-compositions.

\lstset{
	backgroundcolor=\color{lbcolor},
	tabsize=2,
	rulecolor=,
	language=matlab,
        basicstyle=\tiny,
        upquote=true,
        aboveskip={1\baselineskip},
        columns=fixed,
        showstringspaces=false,
        extendedchars=true,
        breaklines=true,
        prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
        frame=single,
        showtabs=false,
        showspaces=false,
        showstringspaces=false,
        identifierstyle=\ttfamily,
        keywordstyle=\color[rgb]{0,0,1},
        commentstyle=\color[rgb]{0.133,0.545,0.133},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
		numbers=left,
}
\begin{figure}[t]
\lstset{language=C,numbersep=4pt}
\begin{center}
\begin{lstlisting}
while( forest_pt[nrindex].shortcut != nrindex ) 
{          
	sref(visited_pt, nvisited++) = nrindex ;
	nrindex = forest_pt[nrindex].shortcut ;
}      
while( nvisited-- ) 
{
	forest_pt [ sref(visited_pt,nvisited) ] .shortcut = nrindex ;
}
\end{lstlisting}
\end{center}
\vspace{-1em}
\caption{Example of MSER loops (lines 27-30, 31-33).}
\label{lst:mser-ex}
\end{figure}


\paragraph{MSER}
\bm{MSER} has two distinct IPC phases according to Figure~\ref{fig:clust}, in the code the first phase, which is the domminant phase, represents the computation of the extremal regions tree.
As seen in Figure~\ref{fig:block_sizes}, the majority of the blocks in MSER are 8 instructions long, meaning that core-compositions will have to load the maximum amount of blocks to fill each core.
The source code of this phase can be found in the Appendix with Listing~\ref{lsting:mser}.
The listing shows how this phase is made out of small tight knit loops.
The while loops found at lines 27-30, 31-33 (both seen in Listing~\ref{lst:mser-ex}, 36-39, 40-42 and 71-75 never run for more than a few iterations.
This entire section of code is full of control-flow causing the blocks to remain very small.

It's also important to note that the average branch-prediction accuracy of \bm{MSER} is of 85\%, as shown in Table~\ref{tab:sd-vbsbpred}.
Recalling Figure~\ref{fig:req_pred} and the fact that the average block size of \bm{MSER} is under 32 instructions, this means that no composition can be fully efficiently used.
Some speedup can still be obtained, as cores in a composition do not need to have all the instruction window full to be able to obtain performance improvements.
However these improvements are minor, and come at a high energy cost.

To get performance out of \bm{MSER}, this major phase would have to be re-written to be less control-flow heavily so that larger blocks can be extracted.

\begin{figure}[t]
\lstset{language=C,numbersep=4pt}
\begin{center}
\begin{lstlisting}
 for (i = 0; i < num_edges; i++) 
 {
        edge *pedge = &edges[ asubsref(indices,i) ];
		a = find(u, pedge->a);
        b = find(u, pedge->b);
        if (a != b) 
		{
            if ((pedge->w <= arrayref(threshold,a)) && (pedge->w <= arrayref(threshold,b))) 
			{
	            join(u, a, b);
	            a = find(u, a);
	            arrayref(threshold,a) = pedge->w + THRESHOLD(u->elts[a].size, c);
            }
        }
    }
\end{lstlisting}
\end{center}
\vspace{-1em}
\caption{Main loop of Multi\_NCut.}
\label{lst:multi_loop}
\vspace{1em}
\end{figure}

\paragraph{Multi-NCut}
\bm{Multi\_NCut} is faced with a similar situation to \bm{MSER}.
Figure~\ref{fig:block_sizes} shows that once again, the majority of the blocks are less than 32 instructions long, in fact, over 50\% of the blocks are of 12 instructions or less.
Table~\ref{tab:sd-vbsbpred} shows that the branch prediction is always correct, yet, with blocks this small, the synchronization cost of executing the blocks on large compositions outdoes any benefit that can be obtained via core-composition.
Listing~\ref{lsting:multi} shows the source code of the main phase of \bm{Multi\_NCut}, the main loop found at lines 49 to 58, also found in Listing~\ref{lst:multi_loop}.
Once again, this loop cannot be efficiently unrolled as the \textit{find} and \textit{join} functions are comprised of small control heavy while loops.
Even by inlining those function calls, the while loops will cause the blocks to be small, and unrolling the while loops will generate more checks which wont help increase the size of the blocks.
For \bm{Multi\_NCut} the problem is that the blocks are too small which will cause the overhead of dispatching multiple blocks across a core-composition inefficient.
This is why the benchmark does not perform significantly better with core-composition without a high increase in energy consumption.


Overall, both benchmarks demonstrate that some loops cannot be adapted to benefit from core-composition.
This is especially true of while loops that cannot be converted into more traditional for loops.
Due to the small size of these loops, the overhead of dispatching multiple blocks over multiple cores simply outweighs the performance benefits.