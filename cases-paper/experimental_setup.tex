The previous section studied the performance potential for core fusion using an analytical model.
This section now presents the experimental setup used for the remaining parts of the chapter where a thorough evaluation of core fusion is conducted with a cycle-level simulator described in Chapter~\ref{chp:setup}.

\subsection{Benchmarks}

\begin{table}[t]
  \small
  \centering
 \begin{tabular} { | l | l | }
 \hline
   \cellcolor[gray]{0.7}Characteristic & \cellcolor[gray]{0.7} Benchmark\\ \hline
    Memory Intensive & Disparity, Tracking\\ \hline
	Computation Intensive & MSER, SVM, SIFT, Localization,Multi NCut\\\hline
	Memory and Computation Intensive & Stitch\\ \hline
   \end{tabular}
  \caption{Characteristics of the benchmarks~\cite{sdvbs}.}\label{tab:sd-vbschar}
\vspace{1em}
  \end{table}

For this chapter the performance of the Dynamic Multicore Processor (DMP) is studied on a set of Vision Benchmarks designed for hardware and compiler research~\cite{sdvbs}.
The San Diego Vision Benchmark suite (SD-VBS) is composed of nine single-threaded C benchmarks ranging from image analysis to motion tracking.
These benchmarks represent state-of-the-art applications in image and vision recognition which are prevalent in embedded systems.
The domain of image analysis and vision recognition is prevalent in multiple commerical and research fields, such as robotics, self-driving cars and even facial recognition in smartphones.

Vision applications are usually designed as software pipelines featuring different passes which will naturally form phases throughout the execution of the program.
The programs typically have regular and simple control flow which enables the formation of large blocks of instructions.
The processor relies on the ability to form large blocks to exploit block level parallelism (BLP) which makes these applications particularly well suited.
As the results will show, the phase length has minimal impact on energy savings when the reconfiguration overhead is low.

The 9 benchmarks are described here:
\begin{itemize}
\item \textbf{Disparity} Computes depth information for a given pair of images.
\vspace{-1em}
\item \textbf{Localization} Estimates position of robot based on its surroundings.
\vspace{-1em}
\item \textbf{MSER} Maximally Stable Extremal Regions, a method used for blob detection in images.
\vspace{-1em}
\item \textbf{Multi NCut} Partitions images into conceptual regions.
\vspace{-1em}
\item \textbf{Sift} Scale invariant feature transform is used to extract and describe items found in an image.
\vspace{-1em}
\item \textbf{Stitch} Combines multiple photographs into a single image.
\vspace{-1em}
\item \textbf{SVM} Support Vector Machine.
\vspace{-1em}
\item \textbf{Texture Synthesis} Creates larger image out of a small sample.
\vspace{-1em}
\item \textbf{Tracking} Extracts motion information from a set of images.
\end{itemize}

and their characteristics in terms of memory/computation intensity are shown in Table~\ref{tab:sd-vbschar}.


\begin{table}[t]
  \small
  \centering
 \begin{tabular} { | l | l | l | l | l | }
 \hline
   \cellcolor[gray]{0.7}Disparity & \cellcolor[gray]{0.7} Localization& \cellcolor[gray]{0.7} MSER& \cellcolor[gray]{0.7} Multi\_NCut& \cellcolor[gray]{0.7} Sift\\ \hline
	VGA  & VGA & CIF  & SIM\_FAST& CIF\\ \hline
	 \cellcolor[gray]{0.7} Stitch & \cellcolor[gray]{0.7} SVM & \cellcolor[gray]{0.7} Text. Synth & \cellcolor[gray]{0.7} Tracking&\\ \hline
	  CIF& CIF& FULLHD& CIF &\\ \hline
	\end{tabular}
  \caption{Datasets used for each of the benchmarks in SD-VBS.}\label{tab:sd-data}
  \vspace{1em}
\end{table}

\paragraph{Input Size}
%Maybe find a way of explaining how long it takes
The SD-VBS benchmark suite comes with a different set of input sizes.
Due to executing these benchmarks on a cycle accurate simulator, executing on large datasets can take a large amount of time.
A single experiment can take up to 6 hours on a single machine (Intel i5 3570k 3GHz, 16 GB of DDR3 RAM), and the average Million instructions per second (MIPS) of the simulator is of 0.1 when only simulating a single core.
In the paper describing each of the applications in the SD-VBS suite~\cite{sdvbs}, Venkata et al. show that increasing the size of the input does not drastically change the phases of each benchmark.
Table~\ref{tab:sd-data} shows the datasets used for the benchmarks in this chapter.
For this chapter, the aim is to have a dataset that leads to executing at least 100 million cycles as this ensures that the caches and branch predictor are warmed up~\cite{dubach13dynamic}.
%Multi_NCut would take 10x time longer on anything else


%\subsection{Architecture and Simulator}

%A cycle-level simulator of an EDGE-based Dynamic Multicore Processor~\cite{e2} is used, whose core pipeline is verified against an RTL implementation within 5\%. 
%This validation is done by running workloads on RTL and comparing the traces cycle-by-cycle with the software simulator.
%The architecture and core fusion mechanics are similar to the work described in~\cite{kim2007tflex,putnam2010e2}.
%he simulator is configured to model a 16 core multiprocessor, with 32 KB private L1 caches, and allow each core to fetch up to 4 blocks of instructions, 
%and issue up to 2 instructions per block for a maximum of 64 blocks in flight.


%\subsection{Compiler}
%Each benchmark is compiled with the Microsoft C++ compiler for EDGE~\cite{e2}, with -O2 optimisations and using instruction predication for hyperblock formation~\cite{smith2006edge}.

\subsection{Measuring Performance and Power}

Five simulations per benchmark are ran, one for each core composition size: 1, 2, 4, 8 and 16.
For each core composition the IPC of the core composition at an interval of 640 committed blocks is recoreded.
640 committed blocks is chosen as it allows each core in a core composition to execute enough blocks before taking the measurement.
This is due to the fact that the highest core composition of 16 cores can execute up to 64 blocks at a time, thus recording performance after 640 blocks allows each core to have executed at least 10 blocks.
Using committed blocks as an interval allows us to easily compare each simulation as the total number of committed blocks does not change even if the core compositions are different.

The EDGE architecture is fundamentally different from the traditional CISC/RISC paradigm and thus, McPAT~\cite{mcpat} cannot be used to model power consumption as it differs from traditional CISC/RISC cores modeled in McPAT.
Instead a coarse grained power model is used where power gating is applied; when a core is not currently being used, it is assumed to be turned off and therefore does not consume energy.